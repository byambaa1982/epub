<!DOCTYPE html>
<html lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.18.1: http://docutils.sourceforge.net/" name="generator"/>
<title>Test 1 — Byambalogy</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" rel="stylesheet" type="text/css"/>
<link href="../_static/togglebutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" rel="preload"/>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
<script>let toggleHintShow = 'Click to show';</script>
<script>let toggleHintHide = 'Click to hide';</script>
<script>let toggleOpenOnPrint = 'true';</script>
<script src="../_static/togglebutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script src="../_static/design-tabs.js"></script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../_static/sphinx-thebe.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'ebook/test_3';</script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="test_1.html" rel="next" title="Test 2"/>
<link href="tests.html" rel="prev" title="Databricks"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<a class="skip-link" href="#main-content">Skip to main content</a>
<input class="sidebar-toggle" id="__primary" name="__primary" type="checkbox"/>
<label class="overlay overlay-primary" for="__primary"></label>
<input class="sidebar-toggle" id="__secondary" name="__secondary" type="checkbox"/>
<label class="overlay overlay-secondary" for="__secondary"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search this book..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<nav class="bd-header navbar navbar-expand-lg bd-navbar">
</nav>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<a class="navbar-brand logo" href="../landing.html">
<img alt="Logo image" class="logo__image only-light" src="../_static/logo.png"/>
<script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
</a></div>
<div class="sidebar-primary-item"><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../landing.html">
                    About this web
                </a>
</li>
</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../courses/courses.html">Courses</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../courses/introduction.html">Python Courses for beginners</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/table_content.html">1. Introduction to Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_2.html">2. Python Data Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_3.html">3. Control Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_4.html">4. Python Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_5.html">5. Lists, Tuples, Dictionary, and Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_6.html">6. String</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/simple_flask_app.html">7. Create a Simple Flask App</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/update_simple_flask_app.html">Update Flask App with more functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../blogs/blogs.html">Blogs</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../blogs/azure_synapse_bq.html">Azure Synapse vs Google Big Query</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/file_and_dir_path.html">Understanding File and Directory Paths in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/Chart.html">Creating Dynamic Bar Charts with Python’s Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/turn_sheets_to_db.html">Mastering Data Flow: Google Sheets to SQLite via Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/wealth.html">The Art of Leveraging Specific Knowledge</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/delta_table.html">Exploring Delta Lake’s Powerful Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/big_data.html">Big Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/cloud_services.html">A Overview of Azure, Google Cloud, and AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/memory_trick.html">Artful Memory Hacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/managed_unmanaged.html">Memorizing Complex Documentation: A Tale of Databricks Tables and Storytelling</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="tests.html">Databricks</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Test 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="test_1.html">Test 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="answers_1.html">Correct Answers</a></li>
<li class="toctree-l2"><a class="reference internal" href="explaination_1.html">Explanations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mongol/main.html">Монгол</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mongol/blog_1.html">Databricks гэж юу?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mongol/blog_2.html">Дата дата л гэнэ яг юу юм бэ?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mongol/sentiment.html">ChatGPT API ашиглан жиргээнд хэрхэн sentiment анализ хийх вэ?</a></li>
</ul>
</li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-bars"></span>
</label></div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="article-header-buttons">
<div class="dropdown dropdown-source-buttons">
<button aria-expanded="false" aria-label="Source repositories" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fab fa-github"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm btn-source-repository-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="https://github.com/byambaa1982/python_cources_for_beginers" target="_blank" title="Source repository">
<span class="btn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="btn__text-container">Repository</span>
</a>
</li>
<li><a class="btn btn-sm btn-source-issues-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="https://github.com/byambaa1982/python_cources_for_beginers/issues/new?title=Issue%20on%20page%20%2Febook/test_3.html&amp;body=Your%20issue%20content%20here." target="_blank" title="Open an issue">
<span class="btn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
</ul>
</div>
<div class="dropdown dropdown-download-buttons">
<button aria-expanded="false" aria-label="Download this page" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fas fa-download"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm btn-download-source-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="../_sources/ebook/test_3.md" target="_blank" title="Download source file">
<span class="btn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="btn__text-container">.md</span>
</a>
</li>
<li>
<button class="btn btn-sm btn-download-pdf-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" onclick="window.print()" title="Print to PDF">
<span class="btn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
<button class="btn btn-sm btn-fullscreen-button" data-bs-placement="bottom" data-bs-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="btn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__secondary" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</label>
</div></div>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>Test 1</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1">Question 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2">Question 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-3">Question 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-4">Question 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-5">Question 5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-6">Question 6</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-7">Question 7</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-8">Question 8</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-9">Question 9</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-10">Question 10</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-11">Question 11</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-12">Question 12</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-13">Question 13</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-14">Question 14</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-15">Question 15</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-16">Question 16</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-17">Question 17</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-18">Question 18</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-19">Question 19</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-20">Question 20</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-21">Question 21</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-22">Question 22</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-23">Question 23</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-24">Question 24</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-25">Question 25</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-26">Question 26</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-27">Question 27</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-28">Question 28</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-29">Question 29</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-30">Question 30</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-31">Question 31</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-32">Question 32</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-33">Question 33</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-34">Question 34</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-35">Question 35</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-36">Question 36</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-37">Question 37</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-38">Question 38</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-39">Question 39</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-40">Question 40</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-41">Question 41</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-42">Question 42</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-43">Question 43</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-44">Question 44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-45">Question 45</a></li>
</ul>
</nav>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" role="main">
<section class="tex2jax_ignore mathjax_ignore" id="test-1">
<h1>Test 1<a class="headerlink" href="#test-1" title="Permalink to this heading">#</a></h1>
<section id="question-1">
<h2>Question 1<a class="headerlink" href="#question-1" title="Permalink to this heading">#</a></h2>
<p>Which of the following is a key feature of a data lakehouse that distinguishes it from traditional data warehouses?</p>
<ul class="simple">
<li><p>A. A data lakehouse relies solely on structured data.</p></li>
<li><p>B. A data lakehouse supports schema enforcement and governance.</p></li>
<li><p>C. A data lakehouse is limited to batch processing only.</p></li>
<li><p>D. A data lakehouse requires separate storage and compute resources.</p></li>
<li><p>E. A data lakehouse is optimized for OLTP workloads.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: B. A data lakehouse supports schema enforcement and governance.</p>
<p>Exam Topic: Databricks Lakehouse Platform</p>
<p>Explanation: A data lakehouse combines the benefits of a data lake and a data warehouse. It supports schema enforcement and governance, which is a key feature that distinguishes it from traditional data warehouses. Unlike data lakes, which often lack governance, and data warehouses, which may not handle unstructured data well, lakehouses provide a unified platform for all types of data while maintaining governance and reliability.</p>
</div>
</section>
<section id="question-2">
<h2>Question 2<a class="headerlink" href="#question-2" title="Permalink to this heading">#</a></h2>
<p>In a Databricks environment, where are the data and compute resources located when processing data?</p>
<ul class="simple">
<li><p>A. In the user’s local environment</p></li>
<li><p>B. In the Databricks workspace</p></li>
<li><p>C. In the cloud provider’s data centers</p></li>
<li><p>D. In the Databricks corporate servers</p></li>
<li><p>E. In a hybrid cloud setup</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C. In the cloud provider’s data centers</p>
<p>Exam Topic: Databricks Lakehouse Platform</p>
<p>Explanation: In a Databricks environment, both the data and compute resources are located in the cloud provider’s data centers. Databricks leverages cloud infrastructure to manage and process data, providing scalability and flexibility.</p>
</div>
</section>
<section id="question-3">
<h2>Question 3<a class="headerlink" href="#question-3" title="Permalink to this heading">#</a></h2>
<p>How can a data lakehouse facilitate the requirements of both real-time analytics and secure, governed batch processing?</p>
<ul class="simple">
<li><p>A. By providing a single, immutable copy of data for all workloads.</p></li>
<li><p>B. By enforcing strict schema-on-read for all data types.</p></li>
<li><p>C. By offering separate storage solutions for each workload type.</p></li>
<li><p>D. By enabling fine-grained security and concurrent data processing.</p></li>
<li><p>E. By relying exclusively on in-memory data processing.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D. By enabling fine-grained security and concurrent data processing.</p>
<p>Exam Topic: Databricks Lakehouse Platform</p>
<p>Explanation: A data lakehouse is designed to support various types of workloads, including real-time analytics and secure, governed batch processing. It enables fine-grained security controls and allows for concurrent data processing, which means that it can handle both unstructured data for analytics and structured data for batch processing with the necessary governance in place.</p>
</div>
</section>
<section id="question-4">
<h2>Question 4<a class="headerlink" href="#question-4" title="Permalink to this heading">#</a></h2>
<p>When should a data engineer opt for an Interactive cluster over a Job cluster in Databricks?</p>
<ul class="simple">
<li><p>A. When running scheduled batch jobs to process data at regular intervals.</p></li>
<li><p>B. When performing exploratory data analysis in a collaborative environment.</p></li>
<li><p>C. When executing a long-running ETL job that requires high reliability.</p></li>
<li><p>D. When triggering a pipeline based on an event or a specific condition.</p></li>
<li><p>E. When optimizing for cost and using spot instances for processing.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: B. When performing exploratory data analysis in a collaborative environment.</p>
<p>Exam Topic: Production Pipelines</p>
<p>Explanation: Interactive clusters are best suited for exploratory data analysis, where collaboration and interactivity are key. They allow multiple users to work together and share insights in real-time. Job clusters, on the other hand, are optimized for running scheduled jobs and automated workflows.</p>
</div>
</section>
<section id="question-5">
<h2>Question 5<a class="headerlink" href="#question-5" title="Permalink to this heading">#</a></h2>
<p>What feature of the Databricks Lakehouse Platform should a data engineer use to manage access control for a Delta table?</p>
<ul class="simple">
<li><p>A. Repos</p></li>
<li><p>B. Unity Catalog</p></li>
<li><p>C. Workflows</p></li>
<li><p>D. SQL Analytics</p></li>
<li><p>E. MLflow</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: B. Unity Catalog</p>
<p>Exam Topic: Data Governance</p>
<p>Explanation: Unity Catalog in Databricks Lakehouse Platform is used to manage access control and governance across all data assets, including Delta tables. It allows data engineers to grant and revoke permissions, ensuring that only authorized users have access to specific data.</p>
</div>
</section>
<section id="question-6">
<h2>Question 6<a class="headerlink" href="#question-6" title="Permalink to this heading">#</a></h2>
<p>What feature of Databricks Notebooks enhances collaboration among data engineers and data scientists?</p>
<ul class="simple">
<li><p>A. The ability to execute notebooks as jobs</p></li>
<li><p>B. The integration with Git for version control</p></li>
<li><p>C. The support for multiple programming languages</p></li>
<li><p>D. The real-time coauthoring capabilities</p></li>
<li><p>E. The built-in data visualization tools</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D. The real-time coauthoring capabilities</p>
<p>Exam Topic: ELT with Spark SQL and Python</p>
<p>Explanation: Databricks Notebooks support real-time coauthoring, allowing multiple users to edit the same notebook simultaneously. This feature enhances collaboration among team members, making it easier to work together on complex data engineering and data science tasks.</p>
</div>
</section>
<section id="question-7">
<h2>Question 7<a class="headerlink" href="#question-7" title="Permalink to this heading">#</a></h2>
<p>How do Databricks Repos support Continuous Integration/Continuous Deployment (CI/CD) in the Databricks Lakehouse Platform?</p>
<ul class="simple">
<li><p>A. By providing a built-in scheduler for job execution</p></li>
<li><p>B. By enabling the use of webhooks to trigger workflows</p></li>
<li><p>C. By allowing users to create and manage Git branches</p></li>
<li><p>D. By automating the deployment of notebooks to production</p></li>
<li><p>E. By integrating with external CI/CD tools for automated testing and deployment</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: E. By integrating with external CI/CD tools for automated testing and deployment</p>
<p>Exam Topic: Production Pipelines</p>
<p>Explanation: Databricks Repos integrate with external CI/CD tools, enabling automated testing and deployment of code changes. This integration helps streamline the development process, ensuring that new features and updates can be rolled out efficiently and reliably.</p>
</div>
</section>
<section id="question-8">
<h2>Question 8<a class="headerlink" href="#question-8" title="Permalink to this heading">#</a></h2>
<p>What is Delta Lake’s primary role within the Databricks ecosystem?</p>
<ul class="simple">
<li><p>A. To provide a machine learning platform for model training and deployment.</p></li>
<li><p>B. To serve as a high-performance query engine for big data analytics.</p></li>
<li><p>C. To act as a transactional storage layer that brings reliability to data lakes.</p></li>
<li><p>D. To offer a distributed computing framework for large-scale data processing.</p></li>
<li><p>E. To function as a real-time streaming engine for event-driven applications.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C. To act as a transactional storage layer that brings reliability to data lakes.</p>
<p>Exam Topic: Incremental Data Processing</p>
<p>Explanation: Delta Lake is an open-source storage layer that brings ACID transactions, scalable metadata handling, and unifies streaming and batch data processing to data lakes. It provides reliability, governance, and performance to data stored in a data lake.</p>
</div>
</section>
<section id="question-9">
<h2>Question 9<a class="headerlink" href="#question-9" title="Permalink to this heading">#</a></h2>
<p>Which SQL DDL command correctly creates a new Delta table with the specified schema, replacing any existing table with the same name?</p>
<ul class="simple">
<li><p>A.</p></li>
</ul>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">new_table</span><span class="w"> </span><span class="p">(</span>
<span class="n">id</span><span class="w"> </span><span class="nb">INT</span><span class="p">,</span>
<span class="n">birthDate</span><span class="w"> </span><span class="nb">DATE</span><span class="p">,</span>
<span class="n">avgRating</span><span class="w"> </span><span class="n">DOUBLE</span>
<span class="p">)</span><span class="w"> </span><span class="k">USING</span><span class="w"> </span><span class="n">DELTA</span>
</pre></div>
</div>
<ul class="simple">
<li><p>B.</p></li>
</ul>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">OR</span><span class="w"> </span><span class="k">REPLACE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">new_table</span><span class="w"> </span><span class="p">(</span>
<span class="n">id</span><span class="w"> </span><span class="nb">INT</span><span class="p">,</span>
<span class="n">birthDate</span><span class="w"> </span><span class="nb">DATE</span><span class="p">,</span>
<span class="n">avgRating</span><span class="w"> </span><span class="n">DOUBLE</span>
<span class="p">)</span><span class="w"> </span><span class="k">USING</span><span class="w"> </span><span class="n">DELTA</span>
</pre></div>
</div>
<ul class="simple">
<li><p>C.</p></li>
</ul>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">DROP</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">new_table</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">new_table</span><span class="w"> </span><span class="p">(</span>
<span class="n">id</span><span class="w"> </span><span class="nb">INT</span><span class="p">,</span>
<span class="n">birthDate</span><span class="w"> </span><span class="nb">DATE</span><span class="p">,</span>
<span class="n">avgRating</span><span class="w"> </span><span class="n">DOUBLE</span>
<span class="p">)</span><span class="w"> </span><span class="k">USING</span><span class="w"> </span><span class="n">DELTA</span>
</pre></div>
</div>
<ul class="simple">
<li><p>D.</p></li>
</ul>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">new_table</span><span class="w"> </span><span class="p">(</span>
<span class="n">id</span><span class="w"> </span><span class="nb">INT</span><span class="p">,</span>
<span class="n">birthDate</span><span class="w"> </span><span class="nb">DATE</span><span class="p">,</span>
<span class="n">avgRating</span><span class="w"> </span><span class="n">DOUBLE</span>
<span class="p">)</span><span class="w"> </span><span class="k">USING</span><span class="w"> </span><span class="n">DELTA</span>
</pre></div>
</div>
<ul class="simple">
<li><p>E.</p></li>
</ul>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">new_table</span><span class="w"> </span><span class="k">RENAME</span><span class="w"> </span><span class="k">TO</span><span class="w"> </span><span class="n">old_table</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">new_table</span><span class="w"> </span><span class="p">(</span>
<span class="n">id</span><span class="w"> </span><span class="nb">INT</span><span class="p">,</span>
<span class="n">birthDate</span><span class="w"> </span><span class="nb">DATE</span><span class="p">,</span>
<span class="n">avgRating</span><span class="w"> </span><span class="n">DOUBLE</span>
<span class="p">)</span><span class="w"> </span><span class="k">USING</span><span class="w"> </span><span class="n">DELTA</span>
</pre></div>
</div>
<div class="toggle docutils container">
<p>Correct Answer: B.</p>
<p>CREATE OR REPLACE TABLE new_table (
id INT,
birthDate DATE,
avgRating DOUBLE
) USING DELTA</p>
<p>Exam Topic: ELT with Spark SQL and Python</p>
<p>Explanation: The <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">OR</span> <span class="pre">REPLACE</span> <span class="pre">TABLE</span></code> command is used to create a new table with the specified schema, and it will replace any existing table with the same name. This ensures that the new table is created with the desired structure, even if a table with the same name already exists.</p>
</div>
</section>
<section id="question-10">
<h2>Question 10<a class="headerlink" href="#question-10" title="Permalink to this heading">#</a></h2>
<p>Which SQL command is used to add new rows to an existing Delta table?</p>
<ul class="simple">
<li><p>A. MERGE INTO</p></li>
<li><p>B. LOAD DATA</p></li>
<li><p>C. INSERT INTO</p></li>
<li><p>D. UPSERT</p></li>
<li><p>E. ADD ROWS</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C. INSERT INTO
Exam Topic: Incremental Data Processing
Explanation: The <code class="docutils literal notranslate"><span class="pre">INSERT</span> <span class="pre">INTO</span></code> SQL command is used to append new rows to an existing table. This command is commonly used in data manipulation operations to add data to tables, including Delta tables in the Databricks environment.</p>
</div>
</section>
<section id="question-11">
<h2>Question 11<a class="headerlink" href="#question-11" title="Permalink to this heading">#</a></h2>
<p>A data engineering team is working with a Delta table and wants to ensure that their queries are as efficient as possible. They are particularly interested in optimizing the retrieval of recent data, which is queried most frequently. What optimization technique should they consider to prioritize the access to the most recent data?</p>
<ul class="simple">
<li><p>A. Partitioning by date</p></li>
<li><p>B. Z-Ordering by date</p></li>
<li><p>C. Data skipping based on date</p></li>
<li><p>D. Increasing the shuffle partitions</p></li>
<li><p>E. Decreasing the size of data files</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A. Partitioning by date</p>
<p>Exam Topic: Incremental Data Processing</p>
<p>Explanation: Partitioning a Delta table by date allows the query engine to read only the relevant partitions that contain the recent data, thus reducing the amount of data scanned and improving query performance.</p>
</div>
</section>
<section id="question-12">
<h2>Question 12<a class="headerlink" href="#question-12" title="Permalink to this heading">#</a></h2>
<p>A data engineer wants to ensure that a database named <code class="docutils literal notranslate"><span class="pre">sales_data</span></code> is created only if it does not already exist in the metastore. The database should be located at the path <code class="docutils literal notranslate"><span class="pre">/data/sales_data</span></code>. Which command should the data engineer use?</p>
<ul class="simple">
<li><p>A. CREATE DATABASE sales_data LOCATION ‘/data/sales_data’;</p></li>
<li><p>B. CREATE DATABASE IF NOT EXISTS sales_data;</p></li>
<li><p>C. CREATE DATABASE IF NOT EXISTS sales_data LOCATION ‘/data/sales_data’;</p></li>
<li><p>D. CREATE DATABASE IF NOT EXISTS sales_data DELTA LOCATION ‘/data/sales_data’;</p></li>
<li><p>E. CREATE DATABASE sales_data DELTA LOCATION ‘/data/sales_data’;</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C. CREATE DATABASE IF NOT EXISTS sales_data LOCATION ‘/data/sales_data’;</p>
<p>Exam Topic: Databricks Lakehouse Platform</p>
<p>Explanation: The <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">DATABASE</span> <span class="pre">IF</span> <span class="pre">NOT</span> <span class="pre">EXISTS</span></code> command followed by the <code class="docutils literal notranslate"><span class="pre">LOCATION</span></code> clause ensures that the database is created only if it does not exist, and specifies the location where the database data should be stored.</p>
</div>
</section>
<section id="question-13">
<h2>Question 13<a class="headerlink" href="#question-13" title="Permalink to this heading">#</a></h2>
<p>A junior data engineer is tasked with creating a managed table named <code class="docutils literal notranslate"><span class="pre">employee_data</span></code> in Spark SQL, where Spark is responsible for managing both the data and metadata. Which command should the senior data engineer recommend?</p>
<ul class="simple">
<li><p>A. CREATE TABLE employee_data (id INT, name STRING) USING parquet OPTIONS (PATH “dbfs:/employee_data”);</p></li>
<li><p>B. CREATE MANAGED TABLE employee_data (id INT, name STRING) USING parquet OPTIONS (PATH “dbfs:/employee_data”);</p></li>
<li><p>C. CREATE MANAGED TABLE employee_data (id INT, name STRING);</p></li>
<li><p>D. CREATE TABLE employee_data (id INT, name STRING) USING DBFS;</p></li>
<li><p>E. CREATE TABLE employee_data (id INT, name STRING);</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C. CREATE MANAGED TABLE employee_data (id INT, name STRING);</p>
<p>Exam Topic: ELT with Spark SQL and Python</p>
<p>Explanation: The <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">MANAGED</span> <span class="pre">TABLE</span></code> command creates a table where Spark manages both the data and metadata. There is no need to specify the storage path as Spark will handle it internally.</p>
</div>
</section>
<section id="question-14">
<h2>Question 14<a class="headerlink" href="#question-14" title="Permalink to this heading">#</a></h2>
<p>A data engineer needs to define a logical structure that can be used to query data from multiple tables without physically storing the combined data. This structure should be accessible across different sessions. What should the data engineer create?</p>
<ul class="simple">
<li><p>A. Global temporary view</p></li>
<li><p>B. Temporary view</p></li>
<li><p>C. Delta Table</p></li>
<li><p>D. Database</p></li>
<li><p>E. Spark SQL Table</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A. Global temporary view</p>
<p>Exam Topic: Production Pipelines</p>
<p>Explanation: A global temporary view is a logical structure that can be used to query data across multiple sessions without physically storing the data. It is session-scoped and will be dropped when the session ends.</p>
</div>
</section>
<section id="question-15">
<h2>Question 15<a class="headerlink" href="#question-15" title="Permalink to this heading">#</a></h2>
<p>A data engineering team is working with Parquet tables that are frequently updated with new data. They want to ensure that their queries reflect the most recent data without being affected by caching issues. What should they do to achieve this?</p>
<ul class="simple">
<li><p>A. Convert the tables to Delta format and enable auto-refresh</p></li>
<li><p>B. Store the tables in a cloud-based storage system with versioning</p></li>
<li><p>C. Refresh the table metadata after each update</p></li>
<li><p>D. Disable caching for the tables</p></li>
<li><p>E. Use streaming to continuously update the tables</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C. Refresh the table metadata after each update</p>
<p>Exam Topic: Data Governance</p>
<p>Explanation: Refreshing the table metadata after each update ensures that the most recent data is available for queries, and it addresses the issue of stale cached data.</p>
</div>
</section>
<section id="question-16">
<h2>Question 16<a class="headerlink" href="#question-16" title="Permalink to this heading">#</a></h2>
<p>A data engineer is creating a new table <code class="docutils literal notranslate"><span class="pre">customer_summary</span></code> from an existing table <code class="docutils literal notranslate"><span class="pre">customer_details</span></code> using the following command:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">customer_summary</span><span class="w"> </span><span class="k">AS</span>
<span class="k">SELECT</span><span class="w"> </span><span class="n">customer_id</span><span class="p">,</span>
<span class="k">SUM</span><span class="p">(</span><span class="n">purchase_amount</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">total_spent</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">customer_details</span>
<span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">customer_id</span><span class="p">;</span>
</pre></div>
</div>
<p>A junior data engineer is curious about why there is no need to specify a schema for the new table. What is the correct explanation?</p>
<ul class="simple">
<li><p>A. The schema is automatically inferred from the result of the SELECT statement.</p></li>
<li><p>B. The schema is copied from the source table <code class="docutils literal notranslate"><span class="pre">customer_details</span></code>.</p></li>
<li><p>C. The schema is not required for tables created with the <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TABLE</span> <span class="pre">AS</span> <span class="pre">SELECT</span></code> command.</p></li>
<li><p>D. All columns in the new table are assumed to be of type STRING.</p></li>
<li><p>E. The new table does not support a schema.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A. The schema is automatically inferred from the result of the SELECT statement.</p>
<p>Exam Topic: ELT with Spark SQL and Python</p>
<p>Explanation: When using the <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TABLE</span> <span class="pre">AS</span> <span class="pre">SELECT</span></code> command, the schema for the new table is automatically inferred from the result of the SELECT statement.</p>
</div>
</section>
<section id="question-17">
<h2>Question 17<a class="headerlink" href="#question-17" title="Permalink to this heading">#</a></h2>
<p>A data engineer is considering the best approach to update a table with new data. They are debating whether to overwrite the existing table or to delete and recreate it. Which of the following is an incorrect reason to choose overwriting over deletion and recreation?</p>
<ul class="simple">
<li><p>A. Overwriting is more efficient as it does not require deleting files.</p></li>
<li><p>B. Overwriting provides a cleaner history for logging and auditing.</p></li>
<li><p>C. Overwriting preserves the old version of the data for Time Travel.</p></li>
<li><p>D. Overwriting is an atomic operation, preventing the table from being left in an inconsistent state.</p></li>
<li><p>E. Overwriting allows for schema evolution without the need to recreate the table.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: B. Overwriting provides a cleaner history for logging and auditing.</p>
<p>Exam Topic: Production Pipelines</p>
<p>Explanation: Overwriting a table does not necessarily provide a cleaner history for logging and auditing. In fact, it may overwrite the history, whereas deleting and recreating a table could potentially provide a clearer demarcation in the table’s history.</p>
</div>
</section>
<section id="question-18">
<h2>Question 18<a class="headerlink" href="#question-18" title="Permalink to this heading">#</a></h2>
<p>A data engineer wants to select unique records from a Delta table named <code class="docutils literal notranslate"><span class="pre">sales_records</span></code>. Which command will achieve this?</p>
<ul class="simple">
<li><p>A. DELETE DUPLICATES FROM sales_records;</p></li>
<li><p>B. SELECT * FROM sales_records WHERE duplicate = False;</p></li>
<li><p>C. SELECT DISTINCT * FROM sales_records;</p></li>
<li><p>D. MERGE INTO sales_records USING (SELECT DISTINCT * FROM sales_records) AS unique_records ON true WHEN NOT MATCHED THEN INSERT *;</p></li>
<li><p>E. MERGE INTO sales_records USING (SELECT DISTINCT * FROM sales_records) AS unique_records;</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C. SELECT DISTINCT * FROM sales_records;</p>
<p>Exam Topic: ELT with Spark SQL and Python</p>
<p>Explanation: The <code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">DISTINCT</span></code> command is used to return only distinct (unique) rows from a table, effectively removing duplicates.</p>
</div>
</section>
<section id="question-19">
<h2>Question 19<a class="headerlink" href="#question-19" title="Permalink to this heading">#</a></h2>
<p>A data engineer needs to join two tables, <code class="docutils literal notranslate"><span class="pre">orders</span></code> and <code class="docutils literal notranslate"><span class="pre">customers</span></code>, on a common column <code class="docutils literal notranslate"><span class="pre">customer_id</span></code>. The result should only include rows that have matching <code class="docutils literal notranslate"><span class="pre">customer_id</span></code> values in both tables. Which SQL command should be used?</p>
<ul class="simple">
<li><p>A. INNER JOIN</p></li>
<li><p>B. FULL OUTER JOIN</p></li>
<li><p>C. LEFT JOIN</p></li>
<li><p>D. RIGHT JOIN</p></li>
<li><p>E. CROSS JOIN</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A. INNER JOIN</p>
<p>Exam Topic: ELT with Spark SQL and Python</p>
<p>Explanation: An INNER JOIN selects records that have matching values in both tables. It is the correct choice when you want to combine rows from two tables and only include the rows with matching keys.</p>
</div>
</section>
<section id="question-20">
<h2>Question 20<a class="headerlink" href="#question-20" title="Permalink to this heading">#</a></h2>
<p>A junior data engineer has a table <code class="docutils literal notranslate"><span class="pre">shopping_carts</span></code> with the following schema:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cart_id</span> <span class="n">STRING</span><span class="p">,</span>
<span class="n">items</span> <span class="n">ARRAY</span><span class="o">&lt;</span><span class="n">STRUCT</span><span class="o">&lt;</span><span class="n">item_id</span><span class="p">:</span><span class="n">STRING</span><span class="p">,</span> <span class="n">quantity</span><span class="p">:</span><span class="n">INT</span><span class="o">&gt;&gt;</span>
</pre></div>
</div>
<p>They need to flatten the <code class="docutils literal notranslate"><span class="pre">items</span></code> array to create a new table with the schema:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cart_id</span> <span class="n">STRING</span><span class="p">,</span>
<span class="n">item_id</span> <span class="n">STRING</span><span class="p">,</span>
<span class="n">quantity</span> <span class="n">INT</span>
</pre></div>
</div>
<p>Which command should they use to achieve this?</p>
<ul class="simple">
<li><p>A. SELECT cart_id, items.item_id, items.quantity FROM shopping_carts;</p></li>
<li><p>B. SELECT cart_id, explode(items) AS (item_id, quantity) FROM shopping_carts;</p></li>
<li><p>C. SELECT cart_id, posexplode(items) AS (item_id, quantity) FROM shopping_carts;</p></li>
<li><p>D. SELECT cart_id, explode_outer(items) AS (item_id, quantity) FROM shopping_carts;</p></li>
<li><p>E. SELECT cart_id, inline(items) AS (item_id, quantity) FROM shopping_carts;</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: B. SELECT cart_id, explode(items) AS (item_id, quantity) FROM shopping_carts;</p>
<p>Exam Topic: ELT with Spark SQL and Python</p>
<p>Explanation: The <code class="docutils literal notranslate"><span class="pre">explode</span></code> function is used to explode an array of structs into a table with multiple rows, with each field of the struct becoming a separate column in the output.</p>
</div>
</section>
<section id="question-21">
<h2>Question 21<a class="headerlink" href="#question-21" title="Permalink to this heading">#</a></h2>
<p>A data engineer needs to flatten a nested JSON structure in a DataFrame <code class="docutils literal notranslate"><span class="pre">events_df</span></code> with the following schema:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">event_id</span> <span class="n">STRING</span><span class="p">,</span>
<span class="n">details</span> <span class="n">STRUCT</span><span class="o">&lt;</span><span class="n">user_id</span><span class="p">:</span><span class="n">STRING</span><span class="p">,</span> <span class="n">timestamp</span><span class="p">:</span><span class="n">TIMESTAMP</span><span class="p">,</span> <span class="n">location</span><span class="p">:</span><span class="n">STRING</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>The engineer wants to create a new DataFrame with the user_id and timestamp fields at the top level. Which command should be used?</p>
<ul class="simple">
<li><p>A. SELECT event_id, details.user_id, details.timestamp FROM events_df;</p></li>
<li><p>B. SELECT event_id, flatten(details) FROM events_df;</p></li>
<li><p>C. SELECT event_id, details[‘user_id’], details[‘timestamp’] FROM events_df;</p></li>
<li><p>D. SELECT event_id, details.user_id as user_id, details.timestamp as timestamp FROM events_df;</p></li>
<li><p>E. SELECT event_id, explode(details) FROM events_df;</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D
Exam Topic: ELT with Spark SQL and Python
Explanation: The correct command to flatten the nested JSON structure and select the <code class="docutils literal notranslate"><span class="pre">user_id</span></code> and <code class="docutils literal notranslate"><span class="pre">timestamp</span></code> fields at the top level is to use the dot notation to access the fields within the <code class="docutils literal notranslate"><span class="pre">details</span></code> struct and alias them appropriately.</p>
</div>
</section>
<section id="question-22">
<h2>Question 22<a class="headerlink" href="#question-22" title="Permalink to this heading">#</a></h2>
<p>A data engineering team needs to automate a Spark SQL query that references a table with a date suffix in its name, such as <code class="docutils literal notranslate"><span class="pre">daily_metrics_20220101</span></code>. The date in the table name should be updated to the current date whenever the query is executed. Which approach should the team take?</p>
<ul class="simple">
<li><p>A. Use a PySpark script with string interpolation to dynamically construct the table name based on the current date.</p></li>
<li><p>B. Create a scheduled job that manually updates the table name in the query each day.</p></li>
<li><p>C. Modify the query to use a static table name without a date suffix.</p></li>
<li><p>D. Implement a UDF (User-Defined Function) to generate the table name dynamically within the query.</p></li>
<li><p>E. Use a configuration file to specify the table name, and update it daily before running the query.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A
Exam Topic: Production Pipelines
Explanation: The most efficient way to automate the process is to use a PySpark script with string interpolation to dynamically construct the table name based on the current date. This allows the query to be run automatically without manual intervention.</p>
</div>
</section>
<section id="question-23">
<h2>Question 23<a class="headerlink" href="#question-23" title="Permalink to this heading">#</a></h2>
<p>A data engineer needs to temporarily expose a PySpark DataFrame <code class="docutils literal notranslate"><span class="pre">sales_df</span></code> to SQL queries for a quick data exploration task. Which command should the engineer execute?</p>
<ul class="simple">
<li><p>A. sales_df.createOrReplaceTempView(“temp_sales”)</p></li>
<li><p>B. sales_df.createGlobalTempView(“temp_sales”)</p></li>
<li><p>C. sales_df.registerTempTable(“temp_sales”)</p></li>
<li><p>D. sales_df.saveAsTable(“temp_sales”)</p></li>
<li><p>E. sales_df.write.saveAsTable(“temp_sales”)</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A
Exam Topic: Databricks Lakehouse Platform
Explanation: The <code class="docutils literal notranslate"><span class="pre">createOrReplaceTempView</span></code> method creates a temporary view that is session-scoped, allowing the data to be queried using SQL for the duration of the Spark session.</p>
</div>
</section>
<section id="question-24">
<h2>Question 24<a class="headerlink" href="#question-24" title="Permalink to this heading">#</a></h2>
<p>A data engineer is tasked with generating a dynamic table name in a Python script, where the table name is composed of a <code class="docutils literal notranslate"><span class="pre">region_code</span></code>, <code class="docutils literal notranslate"><span class="pre">store_number</span></code>, and <code class="docutils literal notranslate"><span class="pre">fiscal_year</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">region_code</span> <span class="pre">=</span> <span class="pre">"west"</span></code>, <code class="docutils literal notranslate"><span class="pre">store_number</span> <span class="pre">=</span> <span class="pre">"250"</span></code>, and <code class="docutils literal notranslate"><span class="pre">fiscal_year</span> <span class="pre">=</span> <span class="pre">"2022"</span></code>, the table name should be <code class="docutils literal notranslate"><span class="pre">west250_sales_2022</span></code>. Which Python command should the engineer use?</p>
<ul class="simple">
<li><p>A. <code class="docutils literal notranslate"><span class="pre">table_name</span> <span class="pre">=</span> <span class="pre">"{}{}{}_sales_{}".format(region_code,</span> <span class="pre">store_number,</span> <span class="pre">fiscal_year)</span></code></p></li>
<li><p>B. <code class="docutils literal notranslate"><span class="pre">table_name</span> <span class="pre">=</span> <span class="pre">region_code</span> <span class="pre">+</span> <span class="pre">store_number</span> <span class="pre">+</span> <span class="pre">"_sales_"</span> <span class="pre">+</span> <span class="pre">fiscal_year</span></code></p></li>
<li><p>C. <code class="docutils literal notranslate"><span class="pre">table_name</span> <span class="pre">=</span> <span class="pre">f"{region_code}{store_number}_sales_{fiscal_year}"</span></code></p></li>
<li><p>D. <code class="docutils literal notranslate"><span class="pre">table_name</span> <span class="pre">=</span> <span class="pre">"%s%s_sales_%s"</span> <span class="pre">%</span> <span class="pre">(region_code,</span> <span class="pre">store_number,</span> <span class="pre">fiscal_year)</span></code></p></li>
<li><p>E. <code class="docutils literal notranslate"><span class="pre">table_name</span> <span class="pre">=</span> <span class="pre">region_code.concat(store_number).concat("_sales_").concat(fiscal_year)</span></code></p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C
Exam Topic: ELT with Spark SQL and Python
Explanation: The f-string (formatted string literal) in Python allows for easy and readable string interpolation, which is ideal for dynamically constructing strings like table names.</p>
</div>
</section>
<section id="question-25">
<h2>Question 25<a class="headerlink" href="#question-25" title="Permalink to this heading">#</a></h2>
<p>A data engineer is attempting to perform a streaming read from a Kafka source using the following code block:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span>
<span class="o">.</span><span class="n">readStream</span>
<span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">"kafka"</span><span class="p">)</span>
<span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"kafka.bootstrap.servers"</span><span class="p">,</span> <span class="s2">"host1:port1,host2:port2"</span><span class="p">)</span>
<span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"subscribe"</span><span class="p">,</span> <span class="s2">"topic1"</span><span class="p">)</span>
<span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The engineer encounters an error stating that the Kafka source is not found. Which of the following changes should be made to the code block to resolve the issue?</p>
<ul class="simple">
<li><p>A. Replace <code class="docutils literal notranslate"><span class="pre">.format("kafka")</span></code> with <code class="docutils literal notranslate"><span class="pre">.format("org.apache.spark.sql.kafka010")</span></code>.</p></li>
<li><p>B. Add <code class="docutils literal notranslate"><span class="pre">.option("startingOffsets",</span> <span class="pre">"earliest")</span></code> after the <code class="docutils literal notranslate"><span class="pre">.option("subscribe",</span> <span class="pre">"topic1")</span></code> line.</p></li>
<li><p>C. Add <code class="docutils literal notranslate"><span class="pre">.option("kafka.metadata.broker.list",</span> <span class="pre">"host1:port1,host2:port2")</span></code> after the <code class="docutils literal notranslate"><span class="pre">.format("kafka")</span></code> line.</p></li>
<li><p>D. Replace <code class="docutils literal notranslate"><span class="pre">.readStream</span></code> with <code class="docutils literal notranslate"><span class="pre">.read</span></code> to perform a batch read instead.</p></li>
<li><p>E. Add <code class="docutils literal notranslate"><span class="pre">.option("failOnDataLoss",</span> <span class="pre">"false")</span></code> after the <code class="docutils literal notranslate"><span class="pre">.option("subscribe",</span> <span class="pre">"topic1")</span></code> line.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A
Exam Topic: Incremental Data Processing
Explanation: The error indicates that the Kafka source is not correctly specified. The format for Kafka in Spark Structured Streaming should be the fully qualified class name <code class="docutils literal notranslate"><span class="pre">org.apache.spark.sql.kafka010</span></code>.</p>
</div>
</section>
<section id="question-26">
<h2>Question 26<a class="headerlink" href="#question-26" title="Permalink to this heading">#</a></h2>
<p>A data engineer is setting up a Structured Streaming job to process data from a Delta Lake table and write the results to another Delta Lake table. The engineer wants to process all available data in a single batch and then stop the stream. Which line of code should be used to configure the trigger for the streaming query?</p>
<ul class="simple">
<li><p>A. <code class="docutils literal notranslate"><span class="pre">.trigger(once=True)</span></code></p></li>
<li><p>B. <code class="docutils literal notranslate"><span class="pre">.trigger(availableNow=True)</span></code></p></li>
<li><p>C. <code class="docutils literal notranslate"><span class="pre">.trigger(processingTime='0</span> <span class="pre">seconds')</span></code></p></li>
<li><p>D. <code class="docutils literal notranslate"><span class="pre">.option("maxFilesPerTrigger",</span> <span class="pre">1)</span></code></p></li>
<li><p>E. <code class="docutils literal notranslate"><span class="pre">.trigger(maxOffsetsPerTrigger=1)</span></code></p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A
Exam Topic: Production Pipelines
Explanation: The <code class="docutils literal notranslate"><span class="pre">trigger(once=True)</span></code> option in Structured Streaming allows the query to process all available data in a single batch and then stop.</p>
</div>
</section>
<section id="question-27">
<h2>Question 27<a class="headerlink" href="#question-27" title="Permalink to this heading">#</a></h2>
<p>A data engineer needs to process only new files that have been added to a directory since the last pipeline run. The directory is shared with other systems, so files cannot be moved or deleted. Which Databricks feature should the engineer use to identify and ingest only the new files?</p>
<ul class="simple">
<li><p>A. Databricks Delta Lake</p></li>
<li><p>B. Databricks Job Scheduler</p></li>
<li><p>C. Databricks File System (DBFS)</p></li>
<li><p>D. Databricks Auto Loader</p></li>
<li><p>E. Databricks Structured Streaming</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D
Exam Topic: Incremental Data Processing
Explanation: Databricks Auto Loader provides a mechanism to incrementally and efficiently process new files in a directory without the need to move or delete them.</p>
</div>
</section>
<section id="question-28">
<h2>Question 28<a class="headerlink" href="#question-28" title="Permalink to this heading">#</a></h2>
<p>A data engineering team is updating their data ingestion pipeline to use Databricks Auto Loader for incremental loading of CSV files. The current code block is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">"cloudFiles"</span><span class="p">)</span>
<span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"cloudFiles.format"</span><span class="p">,</span> <span class="s2">"csv"</span><span class="p">)</span>
<span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"cloudFiles.schemaLocation"</span><span class="p">,</span> <span class="n">schemaLocation</span><span class="p">)</span>
<span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">sourcePath</span><span class="p">))</span>
</pre></div>
</div>
<p>Which of the following changes is necessary to ensure the code block correctly uses Auto Loader for ingesting CSV files?</p>
<ul class="simple">
<li><p>A. No change is required; the code block already uses Auto Loader with the <code class="docutils literal notranslate"><span class="pre">cloudFiles</span></code> format.</p></li>
<li><p>B. Change <code class="docutils literal notranslate"><span class="pre">.format("cloudFiles")</span></code> to <code class="docutils literal notranslate"><span class="pre">.format("csv")</span></code>.</p></li>
<li><p>C. Add <code class="docutils literal notranslate"><span class="pre">.option("header",</span> <span class="pre">"true")</span></code> after the <code class="docutils literal notranslate"><span class="pre">.option("cloudFiles.format",</span> <span class="pre">"csv")</span></code> line.</p></li>
<li><p>D. Add <code class="docutils literal notranslate"><span class="pre">.option("inferSchema",</span> <span class="pre">"true")</span></code> after the <code class="docutils literal notranslate"><span class="pre">.option("cloudFiles.format",</span> <span class="pre">"csv")</span></code> line.</p></li>
<li><p>E. Replace <code class="docutils literal notranslate"><span class="pre">.load(sourcePath)</span></code> with <code class="docutils literal notranslate"><span class="pre">.autoLoader(sourcePath)</span></code>.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A
Exam Topic: Incremental Data Processing
Explanation: The <code class="docutils literal notranslate"><span class="pre">cloudFiles</span></code> format in the code block is the correct format for using Auto Loader in Databricks to ingest CSV files incrementally.</p>
</div>
</section>
<section id="question-29">
<h2>Question 29<a class="headerlink" href="#question-29" title="Permalink to this heading">#</a></h2>
<p>Which of the following data workloads typically uses a Bronze table as its source?</p>
<ul class="simple">
<li><p>A. A job that performs data deduplication and quality checks on raw data</p></li>
<li><p>B. A job that generates business reports from curated data</p></li>
<li><p>C. A job that applies complex transformations to prepare data for machine learning models</p></li>
<li><p>D. A job that performs real-time analytics on processed data</p></li>
<li><p>E. A job that exports data to external systems for further processing</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A
Exam Topic: Data Governance
Explanation: A Bronze table in a data lakehouse architecture typically contains raw data. Jobs that perform initial data cleaning, deduplication, and quality checks would use a Bronze table as their source.</p>
</div>
</section>
<section id="question-30">
<h2>Question 30<a class="headerlink" href="#question-30" title="Permalink to this heading">#</a></h2>
<p>Which of the following data workloads is most likely to use a Silver table as its source?</p>
<ul class="simple">
<li><p>A. A job that performs initial parsing and validation of raw log files</p></li>
<li><p>B. A job that aggregates data to generate daily sales reports</p></li>
<li><p>C. A job that ingests streaming data into the data lakehouse</p></li>
<li><p>D. A job that refines raw data by applying business logic and filtering out bad records</p></li>
<li><p>E. A job that exports curated data to a data warehouse for business intelligence</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D
Exam Topic: Data Governance
Explanation: A Silver table in a data lakehouse architecture typically contains refined data that has been cleaned and processed to apply business logic. Jobs that refine raw data by applying business logic and filtering out bad records would use a Silver table as their source.</p>
</div>
</section>
<section id="question-31">
<h2>Question 31<a class="headerlink" href="#question-31" title="Permalink to this heading">#</a></h2>
<p>Which of the following Structured Streaming queries is correctly calculating the maximum temperature recorded each day for a stream of sensor data?</p>
<ul class="simple">
<li><p>A.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">readStream</span>
    <span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sensorDataSchema</span><span class="p">)</span>
    <span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">sensorDataPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">window</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">"timestamp"</span><span class="p">),</span> <span class="s2">"1 day"</span><span class="p">))</span>
    <span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">"temperature"</span><span class="p">)</span>
    <span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"checkpointLocation"</span><span class="p">,</span> <span class="n">checkpointPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">"complete"</span><span class="p">)</span>
    <span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="s2">"dailyMaxTemperatures"</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>B.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">readStream</span>
    <span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sensorDataSchema</span><span class="p">)</span>
    <span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">sensorDataPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">"timestamp"</span><span class="p">,</span> <span class="s2">"temperature"</span><span class="p">)</span>
    <span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"checkpointLocation"</span><span class="p">,</span> <span class="n">checkpointPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">"update"</span><span class="p">)</span>
    <span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="s2">"dailyTemperatures"</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>C.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">read</span>
    <span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sensorDataSchema</span><span class="p">)</span>
    <span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">sensorDataPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">window</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">"timestamp"</span><span class="p">),</span> <span class="s2">"1 day"</span><span class="p">))</span>
    <span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">"temperature"</span><span class="p">)</span>
    <span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"checkpointLocation"</span><span class="p">,</span> <span class="n">checkpointPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">"complete"</span><span class="p">)</span>
    <span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="s2">"dailyMaxTemperatures"</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>D.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">readStream</span>
    <span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sensorDataSchema</span><span class="p">)</span>
    <span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">sensorDataPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">"temperature"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s2">"temperature"</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">"double"</span><span class="p">))</span>
    <span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"checkpointLocation"</span><span class="p">,</span> <span class="n">checkpointPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">"append"</span><span class="p">)</span>
    <span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="s2">"temperatureCasts"</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>E.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">readStream</span>
    <span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sensorDataSchema</span><span class="p">)</span>
    <span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">sensorDataPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">"temperature"</span><span class="p">)</span><span class="o">.</span><span class="n">isNotNull</span><span class="p">)</span>
    <span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"checkpointLocation"</span><span class="p">,</span> <span class="n">checkpointPath</span><span class="p">)</span>
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">"append"</span><span class="p">)</span>
    <span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="s2">"validTemperatures"</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="toggle docutils container">
<p>Correct Answer: A</p>
<p>Exam Topic: Incremental Data Processing</p>
<p>Explanation:
Option A is the correct answer because it reads from a stream (<code class="docutils literal notranslate"><span class="pre">readStream</span></code>), groups the data by day using a windowing function (<code class="docutils literal notranslate"><span class="pre">groupBy(window(col("timestamp"),</span> <span class="pre">"1</span> <span class="pre">day"))</span></code>), calculates the maximum temperature for each day (<code class="docutils literal notranslate"><span class="pre">max("temperature")</span></code>), and writes the output in complete mode, which is necessary for windowed aggregations that update the full result set as new data arrives.</p>
</div>
</section>
<section id="question-32">
<h2>Question 32<a class="headerlink" href="#question-32" title="Permalink to this heading">#</a></h2>
<p>What feature does Delta Lake provide to ensure data quality and reliability in ELT pipelines?</p>
<ul class="simple">
<li><p>A. Support for ACID transactions</p></li>
<li><p>B. Real-time streaming data ingestion</p></li>
<li><p>C. In-memory data caching for faster processing</p></li>
<li><p>D. Built-in data quality constraints</p></li>
<li><p>E. Automatic handling of schema evolution</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D</p>
<p>Exam Topic: Data Governance</p>
<p>Explanation:</p>
<p>Delta Lake provides built-in data quality constraints, which is a feature for ensuring data quality and reliability in ELT pipelines. These constraints can be used to enforce data quality rules, such as ensuring that certain columns are not null or that the values in a column meet specific criteria.</p>
</div>
</section>
<section id="question-33">
<h2>Question 33<a class="headerlink" href="#question-33" title="Permalink to this heading">#</a></h2>
<p>A data engineer wants to prioritize certain transformations over others in a Delta Live Tables pipeline due to resource constraints. How can this be achieved?</p>
<ul class="simple">
<li><p>A. Use the <code class="docutils literal notranslate"><span class="pre">@dlt.expectation</span></code> decorator to define priority levels.</p></li>
<li><p>B. Manually execute each transformation in the desired order.</p></li>
<li><p>C. Define priority levels in the DLT pipeline’s configuration file.</p></li>
<li><p>D. Use the <code class="docutils literal notranslate"><span class="pre">@dlt.table</span></code> decorator to specify priorities between transformations.</p></li>
<li><p>E. Create separate DLT pipelines for each transformation with different priorities.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C</p>
<p>Exam Topic: Production Pipelines</p>
<p>Explanation:</p>
<p>Defining priority levels in the DLT pipeline’s configuration file allows the data engineer to control the order in which transformations are executed based on resource constraints and dependencies, ensuring that critical transformations are processed first.</p>
</div>
</section>
<section id="question-34">
<h2>Question 34<a class="headerlink" href="#question-34" title="Permalink to this heading">#</a></h2>
<p>When converting a batch query to a Delta Live Tables (DLT) pipeline, what is the correct syntax to create a live table from a CSV file?</p>
<ul class="simple">
<li><p>A. Use <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">OR</span> <span class="pre">REPLACE</span> <span class="pre">LIVE</span> <span class="pre">TABLE</span> <span class="pre">csv_table</span> <span class="pre">AS</span></code> at the beginning of the query</p></li>
<li><p>B. Prefix the CSV file path with <code class="docutils literal notranslate"><span class="pre">dlt.</span></code></p></li>
<li><p>C. Replace the CSV file path with a Delta table reference</p></li>
<li><p>D. Wrap the CSV file path with <code class="docutils literal notranslate"><span class="pre">dlt.read_csv(...)</span></code></p></li>
<li><p>E. Add <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">LIVE</span> <span class="pre">TABLE</span> <span class="pre">csv_table</span> <span class="pre">AS</span></code> at the beginning of the query</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: E</p>
<p>Exam Topic: ELT with Spark SQL and Python</p>
<p>Explanation:</p>
<p>To convert a batch query to a Delta Live Tables pipeline, the data engineer should add <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">LIVE</span> <span class="pre">TABLE</span> <span class="pre">csv_table</span> <span class="pre">AS</span></code> at the beginning of the query. This will define a new live table within the DLT pipeline based on the data read from the CSV file.</p>
</div>
</section>
<section id="question-35">
<h2>Question 35<a class="headerlink" href="#question-35" title="Permalink to this heading">#</a></h2>
<p>In a Delta Live Tables pipeline, a dataset is defined with the following data quality expectation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CONSTRAINT</span> <span class="n">valid_email</span> <span class="n">EXPECT</span> <span class="p">(</span><span class="n">email</span> <span class="n">LIKE</span> <span class="s1">'%@%.%'</span><span class="p">)</span>
</pre></div>
</div>
<p>What happens when a batch of data containing records with invalid email formats is processed?</p>
<ul class="simple">
<li><p>A. Records with invalid email formats are dropped and recorded as invalid in the event log.</p></li>
<li><p>B. Records with invalid email formats are added to the dataset and marked as invalid in the event log.</p></li>
<li><p>C. The entire batch fails and no records are added to the dataset.</p></li>
<li><p>D. Records with invalid email formats are added to the dataset with a warning in the event log.</p></li>
<li><p>E. Records with invalid email formats are sent to a quarantine table for further investigation.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A</p>
<p>Exam Topic: Data Governance</p>
<p>Explanation:</p>
<p>When a batch of data violates the defined data quality expectation in a Delta Live Tables pipeline, the records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log. This ensures that only data meeting the quality constraints is added to the dataset.</p>
</div>
</section>
<section id="question-36">
<h2>Question 36<a class="headerlink" href="#question-36" title="Permalink to this heading">#</a></h2>
<p>What is the behavior of a Delta Live Tables pipeline in Development mode with Triggered Pipeline Mode when it is started with valid definitions and unprocessed data?</p>
<ul class="simple">
<li><p>A. The pipeline updates all datasets once and then enters a paused state, retaining compute resources.</p></li>
<li><p>B. The pipeline continuously updates datasets until manually stopped, retaining compute resources.</p></li>
<li><p>C. The pipeline updates datasets at set intervals until manually stopped, releasing compute resources after each update.</p></li>
<li><p>D. The pipeline updates all datasets once and then shuts down, releasing compute resources.</p></li>
<li><p>E. The pipeline updates datasets at set intervals until manually stopped, retaining compute resources.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D</p>
<p>Exam Topic: Production Pipelines</p>
<p>Explanation:</p>
<p>In Development mode with Triggered Pipeline Mode, when the pipeline is started, it processes the available data once, updates all datasets, and then shuts down. Compute resources are released after the pipeline shuts down.</p>
</div>
</section>
<section id="question-37">
<h2>Question 37<a class="headerlink" href="#question-37" title="Permalink to this heading">#</a></h2>
<p>What strategy can a data engineer employ to ensure nightly job completion while minimizing compute costs when a task in a multi-task job fails intermittently?</p>
<ul class="simple">
<li><p>A. Allocate a dedicated cluster for each task in the job.</p></li>
<li><p>B. Set a retry policy specifically for the task that fails intermittently.</p></li>
<li><p>C. Schedule multiple runs of the job to increase the chance of completion.</p></li>
<li><p>D. Monitor the task in real-time to identify the cause of failure.</p></li>
<li><p>E. Set a retry policy for the entire job.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: B</p>
<p>Exam Topic: Production Pipelines</p>
<p>Explanation:</p>
<p>Setting a retry policy specifically for the task that fails intermittently allows the job to attempt to rerun only the failed task without having to restart the entire job. This minimizes compute costs by not unnecessarily rerunning successful tasks.</p>
</div>
</section>
<section id="question-38">
<h2>Question 38<a class="headerlink" href="#question-38" title="Permalink to this heading">#</a></h2>
<p>To avoid failures in a dependent job caused by the first job not completing on time, what should a data engineer implement?</p>
<ul class="simple">
<li><p>A. Use cluster pools to improve job efficiency.</p></li>
<li><p>B. Implement a retry policy for the first job to ensure faster completion.</p></li>
<li><p>C. Combine both jobs into a single job with task dependencies.</p></li>
<li><p>D. Configure the data to be streamed between the two jobs.</p></li>
<li><p>E. Reduce the output size of the second job to prevent failures.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C</p>
<p>Exam Topic: Production Pipelines</p>
<p>Explanation:</p>
<p>Combining both jobs into a single job with task dependencies ensures that the second task (previously the second job) does not start until the first task (previously the first job) has successfully completed. This eliminates the issue of the second job starting before the first job has finished.</p>
</div>
</section>
<section id="question-39">
<h2>Question 39<a class="headerlink" href="#question-39" title="Permalink to this heading">#</a></h2>
<p>How can a data engineer version control a complex job schedule in Databricks?</p>
<ul class="simple">
<li><p>A. Export the job’s XML configuration from the job’s settings page.</p></li>
<li><p>B. Run the job once on an all-purpose cluster.</p></li>
<li><p>C. Link the job to notebooks in a Databricks Repo.</p></li>
<li><p>D. Export the job’s JSON configuration from the job’s settings page.</p></li>
<li><p>E. Run the job once on a job cluster.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D</p>
<p>Exam Topic: Production Pipelines</p>
<p>Explanation:
Exporting the job’s JSON configuration from the job’s settings page allows the data engineer to version control the job schedule. The JSON file can be committed to a version control system like Git, enabling tracking of changes and rollback if necessary.</p>
</div>
</section>
<section id="question-40">
<h2>Question 40<a class="headerlink" href="#question-40" title="Permalink to this heading">#</a></h2>
<p>What action can the data engineering team take to improve the latency of a data analyst’s Databricks SQL queries if all queries use the same SQL endpoint?</p>
<ul class="simple">
<li><p>A. Enable the Auto Stop feature for the SQL endpoint.</p></li>
<li><p>B. Enable the Serverless feature for the SQL endpoint.</p></li>
<li><p>C. Increase the size of the SQL endpoint’s cluster.</p></li>
<li><p>D. Increase the maximum bound of the SQL endpoint’s auto-scaling range.</p></li>
<li><p>E. Enable the Serverless feature and adjust the Spot Instance Policy to “Cost Optimized.”</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D</p>
<p>Exam Topic: Databricks Lakehouse Platform</p>
<p>Explanation:
Increasing the maximum bound of the SQL endpoint’s auto-scaling range allows the endpoint to scale up to a larger size when needed, which can improve the performance and reduce the latency of the queries.</p>
</div>
</section>
<section id="question-41">
<h2>Question 41<a class="headerlink" href="#question-41" title="Permalink to this heading">#</a></h2>
<p>How can a manager automate the daily update of the results of a Databricks SQL query without manual intervention?</p>
<ul class="simple">
<li><p>A. Schedule the query to run every day from the Jobs UI.</p></li>
<li><p>B. Schedule the query to refresh daily from the query’s page in Databricks SQL.</p></li>
<li><p>C. Schedule the query to run every 12 hours from the Jobs UI.</p></li>
<li><p>D. Schedule the query to refresh daily from the SQL endpoint’s page in Databricks SQL.</p></li>
<li><p>E. Schedule the query to refresh every 12 hours from the SQL endpoint’s page in Databricks SQL.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: B</p>
<p>Exam Topic: Databricks Lakehouse Platform</p>
<p>Explanation:
Scheduling the query to refresh daily from the query’s page in Databricks SQL allows the manager to automate the query execution and ensure that the results are updated each day without manual rerunning.</p>
</div>
</section>
<section id="question-42">
<h2>Question 42<a class="headerlink" href="#question-42" title="Permalink to this heading">#</a></h2>
<p>What can the data engineering team do to be notified if an ELT job has not run in over an hour?</p>
<ul class="simple">
<li><p>A. Set up an Alert for the dashboard to notify them if the runtime exceeds 60 minutes.</p></li>
<li><p>B. Set up an Alert for the query to notify when the ELT job fails.</p></li>
<li><p>C. Set up an Alert for the dashboard to notify when it has not refreshed in 60 minutes.</p></li>
<li><p>D. Set up an Alert for the query to notify them if the runtime exceeds 60 minutes.</p></li>
<li><p>E. This type of alerting is not supported in Databricks.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: C</p>
<p>Exam Topic: Databricks Lakehouse Platform</p>
<p>Explanation:</p>
<p>Setting up an Alert for the dashboard that monitors the ELT job’s runtime can notify the data engineering team if the dashboard has not refreshed in 60 minutes, indicating that the job has not run within the expected timeframe.</p>
</div>
</section>
<section id="question-43">
<h2>Question 43<a class="headerlink" href="#question-43" title="Permalink to this heading">#</a></h2>
<p>Which of the following is NOT a reason why a Databricks SQL dashboard might take a few minutes to update?</p>
<ul class="simple">
<li><p>A. The SQL endpoint used by the queries may need time to start up.</p></li>
<li><p>B. The queries themselves may take a few minutes to execute.</p></li>
<li><p>C. The dashboard’s queries may be waiting for their own clusters to start.</p></li>
<li><p>D. The job updating the dashboard may be using a non-pooled endpoint.</p></li>
<li><p>E. The dashboard is configured to check for new data before updating.</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: E</p>
<p>Exam Topic: Databricks Lakehouse Platform</p>
<p>Explanation:
The reason that the dashboard is configured to check for new data before updating does not explain a delay in the dashboard update. The other options are valid reasons for a delay, such as startup time for endpoints or clusters, or the inherent runtime of the queries.</p>
</div>
</section>
<section id="question-44">
<h2>Question 44<a class="headerlink" href="#question-44" title="Permalink to this heading">#</a></h2>
<p>To grant a new data engineer the ability to query a specific table, which SQL command should be used?</p>
<ul class="simple">
<li><p>A. GRANT SELECT ON TABLE sales TO ‘new.engineer@company.com’;</p></li>
<li><p>B. GRANT USAGE ON TABLE sales TO ‘new.engineer@company.com’;</p></li>
<li><p>C. GRANT SELECT ON DATABASE retail TO ‘new.engineer@company.com’;</p></li>
<li><p>D. GRANT SELECT ON TABLE sales TO USER ‘new.engineer@company.com’;</p></li>
<li><p>E. GRANT READ ON TABLE sales TO ‘new.engineer@company.com’;</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: D</p>
<p>Exam Topic: Data Governance</p>
<p>Explanation:
The correct SQL command to grant a user the ability to query a specific table is <code class="docutils literal notranslate"><span class="pre">GRANT</span> <span class="pre">SELECT</span> <span class="pre">ON</span> <span class="pre">TABLE</span> <span class="pre">sales</span> <span class="pre">TO</span> <span class="pre">USER</span> <span class="pre">'new.engineer@company.com';</span></code>. This grants the SELECT permission on the table <code class="docutils literal notranslate"><span class="pre">sales</span></code> to the user with the specified email address.</p>
</div>
</section>
<section id="question-45">
<h2>Question 45<a class="headerlink" href="#question-45" title="Permalink to this heading">#</a></h2>
<p>To grant a new data engineer full permissions on a table for an ELT project, which SQL command should be used?</p>
<ul class="simple">
<li><p>A. GRANT ALL PRIVILEGES ON TABLE sales TO ‘new.engineer@company.com’;</p></li>
<li><p>B. GRANT ALL ON TABLE sales TO ‘new.engineer@company.com’;</p></li>
<li><p>C. GRANT ALL PRIVILEGES ON DATABASE retail TO ‘new.engineer@company.com’;</p></li>
<li><p>D. GRANT ALL ON TABLE sales TO USER ‘new.engineer@company.com’;</p></li>
<li><p>E. GRANT CONTROL ON TABLE sales TO ‘new.engineer@company.com’;</p></li>
</ul>
<div class="toggle docutils container">
<p>Correct Answer: A</p>
<p>Exam Topic: Data Governance</p>
<p>Explanation:
The correct SQL command to grant a user full permissions on a table is <code class="docutils literal notranslate"><span class="pre">GRANT</span> <span class="pre">ALL</span> <span class="pre">PRIVILEGES</span> <span class="pre">ON</span> <span class="pre">TABLE</span> <span class="pre">sales</span> <span class="pre">TO</span> <span class="pre">'new.engineer@company.com';</span></code>. This grants all available privileges on the table <code class="docutils literal notranslate"><span class="pre">sales</span></code> to the user with the specified email address, allowing them to fully manage the table.</p>
</div>
</section>
</section>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ebook"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="tests.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Databricks</p>
</div>
</a>
<a class="right-next" href="test_1.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Test 2</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div></div>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> Contents
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1">Question 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2">Question 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-3">Question 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-4">Question 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-5">Question 5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-6">Question 6</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-7">Question 7</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-8">Question 8</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-9">Question 9</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-10">Question 10</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-11">Question 11</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-12">Question 12</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-13">Question 13</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-14">Question 14</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-15">Question 15</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-16">Question 16</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-17">Question 17</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-18">Question 18</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-19">Question 19</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-20">Question 20</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-21">Question 21</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-22">Question 22</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-23">Question 23</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-24">Question 24</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-25">Question 25</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-26">Question 26</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-27">Question 27</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-28">Question 28</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-29">Question 29</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-30">Question 30</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-31">Question 31</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-32">Question 32</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-33">Question 33</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-34">Question 34</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-35">Question 35</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-36">Question 36</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-37">Question 37</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-38">Question 38</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-39">Question 39</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-40">Question 40</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-41">Question 41</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-42">Question 42</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-43">Question 43</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-44">Question 44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-45">Question 45</a></li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
<div class="bd-footer-content__inner container">
<div class="footer-item">
<p class="component-author">
By Byamba Enkhbat
</p>
</div>
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2022.
      <br/>
</p>
</div>
<div class="footer-item">
</div>
<div class="footer-item">
</div>
</div>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>
<footer class="bd-footer">
</footer>
</body>
</html>