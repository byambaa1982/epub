{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "- E. A data lakehouse enables both batch and streaming analytics.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "A significant advantage of a data lakehouse over a traditional data warehouse is its ability to handle both batch and streaming analytics. Traditional data warehouses were primarily designed for batch processing, and they might struggle with real-time data ingestion and streaming analytics. On the other hand, a data lakehouse can accommodate both without any significant performance impact. The other options either are capabilities of both systems or don't accurately describe the features of a data lakehouse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "- A. Data plane\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The Data Plane in Databricks hosts the driver and worker nodes of a Databricks-managed cluster. It is responsible for the execution of jobs and computation tasks. The Control Plane, on the other hand, is where the user interface, REST API, job launching, and cluster management operations take place. The Databricks Filesystem, JDBC data source, and Databricks web application are not physical locations, so they don't host driver or worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "- D. A data lakehouse stores unstructured data and is ACID-compliant.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "A data lakehouse, such as Delta Lake, can meet the needs of both workloads because it can handle unstructured data (like video files for machine learning workloads) and is ACID-compliant (which is a key requirement for highly audited batch ETL/ELT workloads). ACID compliance ensures transactional reliability, which is essential for ETL/ELT processes.\n",
    "\n",
    "The other options do not fully capture the dual functionality that a data lakehouse can provide for both types of workloads. Option B and C are incorrect because even though they are features of a data lakehouse, they do not directly address the needs of both machine learning and ETL/ELT workloads. Option A is incorrect because data lakehouses do require data modeling. Lastly, option E is also incorrect because a data lakehouse does not have to fully exist in the cloud - it can be implemented on-premises or in a hybrid environment as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "- C. An automated workflow needs to be run every 30 minutes.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Job clusters are specifically designed for running scheduled or automated tasks, like an automated workflow that needs to run every 30 minutes (option C). They are spun up when the job starts and are terminated when the job finishes, thus providing efficient use of resources for tasks that do not require a constantly running cluster.\n",
    "\n",
    "On the other hand, all-purpose clusters are meant for interactive analysis and collaboration - they are not the best choice for running scheduled jobs, as they are not automatically terminated when a task finishes and therefore could incur unnecessary costs if not manually shut down. This is why options A, B, D, and E are not the best use cases for job clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "- C. Data Explorer\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Data Explorer in Databricks is the place where data engineers or data scientists can manage the permissions on tables. You can grant, revoke, or list permissions on your Delta tables for individual users or groups. In this scenario, the data engineer would use Data Explorer to give SELECT permission on the Delta table to the data analysts.\n",
    "\n",
    "Other Options Explanation:\n",
    "\n",
    "- A. Repos: Databricks Repos are used for version control and collaboration on code in notebooks, not for managing permissions on Delta tables.\n",
    "\n",
    "- B. Jobs: The Databricks Jobs service is used for scheduling and running jobs, not for managing permissions on Delta tables.\n",
    "\n",
    "- D. Databricks Filesystem: Databricks Filesystem (DBFS) is a distributed file system installed on Databricks clusters. It's used for storing data, not for managing permissions on Delta tables.\n",
    "\n",
    "- E. Dashboards: Dashboards in Databricks are used for visualizing and sharing results, not for managing permissions on Delta tables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 27\n",
    "\n",
    "- E. Auto Loader \n",
    "\n",
    "The data engineer can use Auto Loader to solve this problem. Auto Loader, a feature available in Databricks, is designed to simplify the process of incrementally ingesting data, such as new files in a directory. `Auto Loader` keeps track of new files as they arrive, and it automatically processes these files. It's a robust and low-maintenance option for ingesting incrementally updated or new data.\n",
    "\n",
    "Please note, while `Delta Lake (B)` is a technology that can provide features like ACID transactions, versioning, and schema enforcement on data lakes, it doesn't directly address the problem of identifying and processing only new files since the last pipeline run. Similarly, `Databricks SQL (A)`, `Unity Catalog (C)`, and `Data Explorer (D`) are not directly targeted at this specific problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 28\n",
    "\n",
    "- C. There is no change required. The inclusion of `format(\"cloudFiles\")` enables the use of Auto Loader.\n",
    "\n",
    "\n",
    "In Databricks, the Auto Loader feature is utilized through the \"cloudFiles\" format option. So, the given code block is already configured to use Auto Loader. Therefore, no changes are needed in the code.\n",
    "\n",
    "Other Options Explanation:\n",
    "\n",
    "- A. The data engineer needs to change the `format(\"cloudFiles\")` line to `format(\"autoLoader\")`: This is incorrect because Auto Loader is invoked using the \"cloudFiles\" format, not \"autoLoader\".\n",
    "\n",
    "- B. There is no change required. Databricks automatically uses Auto Loader for streaming reads: This is incorrect. Auto Loader needs to be explicitly invoked in the readStream command using the \"cloudFiles\" format.\n",
    "\n",
    "- D. The data engineer needs to add the .autoLoader line before the `.load(sourcePath)` line: This is incorrect as there is no .autoLoader option available. The Auto Loader is enabled using the `format(\"cloudFiles\")` command.\n",
    "\n",
    "- E. There is no change required. The data engineer needs to ask their administrator to turn on Auto Loader: This is incorrect. The Auto Loader can be enabled directly in the code by the data engineer without needing any administrative privileges."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 29 \n",
    "\n",
    "- E. A job that enriches data by parsing its timestamps into a human-readable format.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Bronze tables in Databricks' Lakehouse pattern are used for storing raw, unprocessed data. However, the process of converting timestamps into a more human-readable format is a form of data enrichment, which typically happens at this \"Bronze\" level. The enriched data is then stored in Silver tables for further processing or analysis.\n",
    "\n",
    "Other Options Explanation:\n",
    "\n",
    "- A. A job that aggregates cleaned data to create standard summary statistics: This would likely utilize a Silver or Gold table, which contain processed and cleaned data ready for analysis.\n",
    "\n",
    "- B. A job that queries aggregated data to publish key insights into a dashboard: This would likely involve a Gold table, which contains data that has been cleaned, processed, and possibly aggregated, and is used for reporting and analytics.\n",
    "\n",
    "- C. A job that ingests raw data from a streaming source into the Lakehouse: This is a correct statement, however, it does not pertain to the task of parsing timestamps which the question focuses on.\n",
    "\n",
    "- D. A job that develops a feature set for a machine learning application: Depending on the specifics, this could potentially involve any stage of data, but typically this would be either Silver or Gold data, which has been cleaned and processed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 30\n",
    "\n",
    "D. A job that aggregates cleaned data to create standard summary statistics\n",
    "\n",
    "Explanation:\n",
    "\n",
    "In the Databricks Lakehouse paradigm, a Silver table is typically used to store clean and processed data that can be used for various types of transformations, including the calculation of aggregated statistics. Therefore, a job that aggregates cleaned data to create standard summary statistics would utilize a Silver table as its source.\n",
    "\n",
    "Other Options Explanation:\n",
    "\n",
    "A. A job that enriches data by parsing its timestamps into a human-readable format: This operation could be performed at either the Bronze or Silver level, depending on the specific requirements of the pipeline. However, as it's a form of data cleaning or enrichment, it's more likely to be performed at the Bronze level.\n",
    "\n",
    "B. A job that queries aggregated data that already feeds into a dashboard: This type of operation typically involves Gold tables, which are used for high-level reporting and analytics.\n",
    "\n",
    "C. A job that ingests raw data from a streaming source into the Lakehouse: This operation would involve a Bronze table, which is used for the initial ingestion of raw data.\n",
    "\n",
    "E. A job that cleans data by removing malformatted records: While this could potentially occur at the Silver level, the initial round of data cleaning often happens at the Bronze level before the data is loaded into a Silver table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 31\n",
    "\n",
    "C.  \n",
    "\n",
    "```python\n",
    "(spark.table(\"sales\")\n",
    ".withColumn(\"avgPrice\", col(\"sales\") / col(\"units\"))\n",
    ".writeStream\n",
    ".option(\"checkpointLocation\", checkpointPath)\n",
    ".outputMode(\"append\")\n",
    ".table(\"cleanedSales\")\n",
    ")\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The Bronze-Silver-Gold architecture in data management is a tiered framework for data processing and storage. Bronze represents raw, unprocessed data, Silver represents cleansed and enriched data, while Gold represents aggregated and business-ready data.\n",
    "\n",
    "The statement under option C takes a stream of data from the \"sales\" Bronze table, enriches it by calculating the \"avgPrice\", and writes it to the \"cleanedSales\" table, effectively transforming it to a Silver table. The transition from raw data to enriched data is generally considered a transition from a Bronze to a Silver table, which is why this option is the correct answer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 32 \n",
    "\n",
    "- A. The ability to declare and maintain data table dependencies\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Delta Live Tables offers several advantages over standard data pipelines that use Spark and Delta Lake on Databricks. One such advantage is the ability to declare and maintain dependencies between tables. This is useful when the output of one table is used as the input to another. It makes the pipeline more maintainable and resilient to changes.\n",
    "\n",
    "Option B, the ability to write pipelines in Python and/or SQL, is not unique to Delta Live Tables. You can also write Spark and Delta Lake pipelines in these languages.\n",
    "\n",
    "Option C, accessing previous versions of data tables, is a feature of Delta Lake (through Delta Time Travel), not specific to Delta Live Tables.\n",
    "\n",
    "Option D, the ability to automatically scale compute resources, is a feature of Databricks, not specific to Delta Live Tables.\n",
    "\n",
    "Option E, performing batch and streaming queries, is possible with both Spark Structured Streaming and Delta Lake, not specific to Delta Live Tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 33\n",
    "\n",
    "- B. They need to create a Delta Live Tables pipeline from the Jobs page.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Here are the steps on how to create a Delta Live Tables pipeline from the Jobs page in Databricks:\n",
    "\n",
    "1. Go to the Jobs page in Databricks.\n",
    "2. Click on the \"Create Job\" button.\n",
    "3. Select \"Delta Live Tables\" as the job type.\n",
    "4. Select the notebooks that you want to include in the pipeline.\n",
    "5. Specify the order in which you want the notebooks to be executed.\n",
    "6. Click on the \"Create\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 34\n",
    "\n",
    "- B. They need to add a CREATE LIVE TABLE table_name AS line at the beginning of the query.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Delta Live Tables (DLT) supports SQL and Python syntax. To define a table in SQL, one has to use the CREATE LIVE TABLE statement before defining the query. Hence, to convert the query into a DLT compatible query, we would add the CREATE LIVE TABLE command at the beginning.\n",
    "\n",
    "It's worth mentioning that as of my knowledge cutoff in September 2021, this is the correct information. Always refer to the most recent Databricks documentation for the most up-to-date processes and procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 35\n",
    "\n",
    "- A. Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The CONSTRAINT clause in Delta Live Tables sets an expectation on the dataset. If the action to be taken upon violation of the expectation is not explicitly specified, the default action, which is 'warn', is applied. With the 'warn' action, the records that do not meet the expectation are written to the target dataset, and a failure metric for the dataset is recorded in the event log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 36 \n",
    "\n",
    "- A. Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "In Continuous Pipeline Mode with Production mode enabled, all tables in the pipeline are updated continuously until the pipeline is manually stopped. When 'Start' is clicked, the pipeline initiates and continues to run, processing incoming data in real-time. Compute resources are allocated for the entire duration of the pipeline execution, thus ensuring optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 37 \n",
    "\n",
    "- D. They can institute a retry policy for the task that periodically fails\n",
    "\n",
    "Explanation:\n",
    "\n",
    "A retry policy can be instituted at the task level in Databricks. This would allow the specific task that is failing to be retried without having to rerun the entire job, thus minimizing compute costs. The other options, such as retrying the entire job or setting the job to run multiple times, would increase compute costs as more tasks would be run than necessary. Observing the task as it runs might help determine why it is failing, but it won't ensure the job completes each night. Finally, utilizing a jobs cluster for each of the tasks in the job would not necessarily address the issue of the task failing and could lead to increased compute costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 38\n",
    "\n",
    "- A. They can utilize multiple tasks in a single job with a linear dependency\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The most reliable solution to this problem would be to set up multiple tasks within a single job with a linear dependency. This approach ensures that the second task (which was previously the second job) will not start until the first task (previously the first job) has successfully completed. This removes the problem of the second job starting before the first has completed. The other options, like using cluster pools, setting a retry policy on the first job, or limiting the size of the output in the second job, do not directly address the issue of the second job starting before the first job has finished. The option to set up the data to stream from the first job to the second job is not a typical way to ensure job dependencies in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 39\n",
    "\n",
    "- C. They can download the JSON description of the Job from the Job’s page.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Databricks provides the option to download a JSON description of the job configuration from the job’s page. This JSON description can be version controlled, and can also be used to recreate the job programmatically using the Databricks Jobs API. This provides a way to have version-controllable configuration of the Job's schedule. Other options like linking the job to notebooks that are a part of a Databricks Repo, submitting the job on a job cluster or all-purpose cluster, or downloading an XML description of the job do not offer the same level of version control for a job’s schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 40\n",
    "- C. They can increase the cluster size of the SQL endpoint. \n",
    "\n",
    "Explanation:\n",
    "\n",
    "Databricks SQL Endpoint is the compute that is used to execute SQL queries in Databricks. The performance of the SQL queries can be directly improved by increasing the size of the underlying cluster used by the SQL endpoint. The cluster size in Databricks is defined by the number and type of machines (known as nodes) in the cluster. A larger cluster can handle more data and perform operations faster because the computations are distributed among more machines. Therefore, by increasing the cluster size of the SQL endpoint, the data engineering team can improve the performance of the data analyst’s queries. Other options mentioned might not have a direct impact on the performance of the SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 41 \n",
    "\n",
    "- B. They can schedule the query to refresh every 1 day from the query’s page in\n",
    "Databricks SQL.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "In Databricks SQL, users have the ability to schedule their SQL queries to run at specific intervals. This can be very useful for scenarios where the results of the query need to be updated periodically. In this case, the engineering manager can schedule the query to refresh every 1 day directly from the query's page in Databricks SQL. This will ensure that the query is executed automatically every day, updating the results without requiring the manager to manually rerun the query each time. Other options such as scheduling from the Jobs UI or from the SQL endpoint's page may not be applicable as they pertain to different Databricks features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 42\n",
    "\n",
    "- D. They can set up an Alert for the query to notify them if the returned value is greater\n",
    "than 60.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Databricks SQL allows you to create Alerts on specific queries. In this scenario, the data engineering team can create an Alert on the query that monitors the ELT job runtime. The Alert can be set to trigger a notification when the query result (the number of minutes since the job's most recent runtime) exceeds 60. This way, the team will be notified if the ELT job has not run in over an hour. Other options such as alerting on dashboard conditions or job failures may not provide the specific information needed in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 43\n",
    "\n",
    "- D. The Job associated with updating the dashboard might be using a non-pooled\n",
    "endpoint.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Databricks SQL dashboards do not directly depend on Jobs. Rather, they execute SQL queries on SQL Endpoints (either serverless or a cluster-backed endpoint). Therefore, the speed at which a dashboard refreshes is independent of whether the Jobs are using pooled or non-pooled endpoints. The other options, such as the SQL endpoint needing time to start up, the inherent complexity of the queries, or the queries checking for new data before executing, could indeed contribute to the delay in updating the dashboard. The fifth option, E, is incorrect because individual queries in a dashboard do not connect to their own, separate Databricks clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 44\n",
    "\n",
    "- C. GRANT SELECT ON TABLE sales TO new.engineer@company.com;\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The GRANT SELECT command is used to give a user permission to read a database table. Therefore, the command GRANT SELECT ON TABLE sales TO new.engineer@company.com; would grant the new data engineer the permissions needed to query the 'sales' table. The USAGE privilege doesn't allow data access, it only allows the user to access objects in the database. CREATE privilege would allow the user to create objects in the database, but not necessarily read data from existing tables. Options D and E are incorrectly formulated, as they try to grant privileges on a table named \"new.engineer@company.com\" to a user or role called 'sales'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 45 \n",
    "\n",
    "- A. GRANT ALL PRIVILEGES ON TABLE sales TO new.engineer@company.com;\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The GRANT ALL PRIVILEGES command is used to give a user full permissions to an object, such as a table. In this case, the command GRANT ALL PRIVILEGES ON TABLE sales TO new.engineer@company.com; would grant the new data engineer all necessary permissions to fully manage the 'sales' table. The other options either grant insufficient privileges (e.g., USAGE or SELECT only) or are not correctly formatted SQL commands."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}