{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "- C. Data Explorer\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Data Explorer in Databricks is the place where data engineers or data scientists can manage the permissions on tables. You can grant, revoke, or list permissions on your Delta tables for individual users or groups. In this scenario, the data engineer would use Data Explorer to give SELECT permission on the Delta table to the data analysts.\n",
    "\n",
    "Other Options Explanation:\n",
    "\n",
    "- A. Repos: Databricks Repos are used for version control and collaboration on code in notebooks, not for managing permissions on Delta tables.\n",
    "\n",
    "- B. Jobs: The Databricks Jobs service is used for scheduling and running jobs, not for managing permissions on Delta tables.\n",
    "\n",
    "- D. Databricks Filesystem: Databricks Filesystem (DBFS) is a distributed file system installed on Databricks clusters. It's used for storing data, not for managing permissions on Delta tables.\n",
    "\n",
    "- E. Dashboards: Dashboards in Databricks are used for visualizing and sharing results, not for managing permissions on Delta tables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 27\n",
    "\n",
    "- E. Auto Loader \n",
    "\n",
    "The data engineer can use Auto Loader to solve this problem. Auto Loader, a feature available in Databricks, is designed to simplify the process of incrementally ingesting data, such as new files in a directory. `Auto Loader` keeps track of new files as they arrive, and it automatically processes these files. It's a robust and low-maintenance option for ingesting incrementally updated or new data.\n",
    "\n",
    "Please note, while `Delta Lake (B)` is a technology that can provide features like ACID transactions, versioning, and schema enforcement on data lakes, it doesn't directly address the problem of identifying and processing only new files since the last pipeline run. Similarly, `Databricks SQL (A)`, `Unity Catalog (C)`, and `Data Explorer (D`) are not directly targeted at this specific problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 28\n",
    "\n",
    "- C. There is no change required. The inclusion of `format(\"cloudFiles\")` enables the use of Auto Loader.\n",
    "\n",
    "\n",
    "In Databricks, the Auto Loader feature is utilized through the \"cloudFiles\" format option. So, the given code block is already configured to use Auto Loader. Therefore, no changes are needed in the code.\n",
    "\n",
    "Other Options Explanation:\n",
    "\n",
    "- A. The data engineer needs to change the `format(\"cloudFiles\")` line to `format(\"autoLoader\")`: This is incorrect because Auto Loader is invoked using the \"cloudFiles\" format, not \"autoLoader\".\n",
    "\n",
    "- B. There is no change required. Databricks automatically uses Auto Loader for streaming reads: This is incorrect. Auto Loader needs to be explicitly invoked in the readStream command using the \"cloudFiles\" format.\n",
    "\n",
    "- D. The data engineer needs to add the .autoLoader line before the `.load(sourcePath)` line: This is incorrect as there is no .autoLoader option available. The Auto Loader is enabled using the `format(\"cloudFiles\")` command.\n",
    "\n",
    "- E. There is no change required. The data engineer needs to ask their administrator to turn on Auto Loader: This is incorrect. The Auto Loader can be enabled directly in the code by the data engineer without needing any administrative privileges."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 29 \n",
    "\n",
    "- E. A job that enriches data by parsing its timestamps into a human-readable format.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Bronze tables in Databricks' Lakehouse pattern are used for storing raw, unprocessed data. However, the process of converting timestamps into a more human-readable format is a form of data enrichment, which typically happens at this \"Bronze\" level. The enriched data is then stored in Silver tables for further processing or analysis.\n",
    "\n",
    "Other Options Explanation:\n",
    "\n",
    "- A. A job that aggregates cleaned data to create standard summary statistics: This would likely utilize a Silver or Gold table, which contain processed and cleaned data ready for analysis.\n",
    "\n",
    "- B. A job that queries aggregated data to publish key insights into a dashboard: This would likely involve a Gold table, which contains data that has been cleaned, processed, and possibly aggregated, and is used for reporting and analytics.\n",
    "\n",
    "- C. A job that ingests raw data from a streaming source into the Lakehouse: This is a correct statement, however, it does not pertain to the task of parsing timestamps which the question focuses on.\n",
    "\n",
    "- D. A job that develops a feature set for a machine learning application: Depending on the specifics, this could potentially involve any stage of data, but typically this would be either Silver or Gold data, which has been cleaned and processed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 30\n",
    "\n",
    "D. A job that aggregates cleaned data to create standard summary statistics\n",
    "\n",
    "Explanation:\n",
    "\n",
    "In the Databricks Lakehouse paradigm, a Silver table is typically used to store clean and processed data that can be used for various types of transformations, including the calculation of aggregated statistics. Therefore, a job that aggregates cleaned data to create standard summary statistics would utilize a Silver table as its source.\n",
    "\n",
    "Other Options Explanation:\n",
    "\n",
    "A. A job that enriches data by parsing its timestamps into a human-readable format: This operation could be performed at either the Bronze or Silver level, depending on the specific requirements of the pipeline. However, as it's a form of data cleaning or enrichment, it's more likely to be performed at the Bronze level.\n",
    "\n",
    "B. A job that queries aggregated data that already feeds into a dashboard: This type of operation typically involves Gold tables, which are used for high-level reporting and analytics.\n",
    "\n",
    "C. A job that ingests raw data from a streaming source into the Lakehouse: This operation would involve a Bronze table, which is used for the initial ingestion of raw data.\n",
    "\n",
    "E. A job that cleans data by removing malformatted records: While this could potentially occur at the Silver level, the initial round of data cleaning often happens at the Bronze level before the data is loaded into a Silver table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 31\n",
    "\n",
    "C.  \n",
    "\n",
    "```python\n",
    "(spark.table(\"sales\")\n",
    ".withColumn(\"avgPrice\", col(\"sales\") / col(\"units\"))\n",
    ".writeStream\n",
    ".option(\"checkpointLocation\", checkpointPath)\n",
    ".outputMode(\"append\")\n",
    ".table(\"cleanedSales\")\n",
    ")\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The Bronze-Silver-Gold architecture in data management is a tiered framework for data processing and storage. Bronze represents raw, unprocessed data, Silver represents cleansed and enriched data, while Gold represents aggregated and business-ready data.\n",
    "\n",
    "The statement under option C takes a stream of data from the \"sales\" Bronze table, enriches it by calculating the \"avgPrice\", and writes it to the \"cleanedSales\" table, effectively transforming it to a Silver table. The transition from raw data to enriched data is generally considered a transition from a Bronze to a Silver table, which is why this option is the correct answer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionn 32 \n",
    "\n",
    "- A. The ability to declare and maintain data table dependencies\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Delta Live Tables offers several advantages over standard data pipelines that use Spark and Delta Lake on Databricks. One such advantage is the ability to declare and maintain dependencies between tables. This is useful when the output of one table is used as the input to another. It makes the pipeline more maintainable and resilient to changes.\n",
    "\n",
    "Option B, the ability to write pipelines in Python and/or SQL, is not unique to Delta Live Tables. You can also write Spark and Delta Lake pipelines in these languages.\n",
    "\n",
    "Option C, accessing previous versions of data tables, is a feature of Delta Lake (through Delta Time Travel), not specific to Delta Live Tables.\n",
    "\n",
    "Option D, the ability to automatically scale compute resources, is a feature of Databricks, not specific to Delta Live Tables.\n",
    "\n",
    "Option E, performing batch and streaming queries, is possible with both Spark Structured Streaming and Delta Lake, not specific to Delta Live Tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}