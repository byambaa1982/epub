<!DOCTYPE html>
<html lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6R7KD1FNMR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6R7KD1FNMR');
</script>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.18.1: http://docutils.sourceforge.net/" name="generator"/>
<title>Explanations — Byambalogy</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" rel="stylesheet" type="text/css"/>
<link href="../_static/togglebutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" rel="preload"/>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
<script>let toggleHintShow = 'Click to show';</script>
<script>let toggleHintHide = 'Click to hide';</script>
<script>let toggleOpenOnPrint = 'true';</script>
<script src="../_static/togglebutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script src="../_static/design-tabs.js"></script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../_static/sphinx-thebe.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'ebook/explaination_1';</script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../mongol/main.html" rel="next" title="Монгол"/>
<link href="answers_1.html" rel="prev" title="Correct Answers"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<a class="skip-link" href="#main-content">Skip to main content</a>
<input class="sidebar-toggle" id="__primary" name="__primary" type="checkbox"/>
<label class="overlay overlay-primary" for="__primary"></label>
<input class="sidebar-toggle" id="__secondary" name="__secondary" type="checkbox"/>
<label class="overlay overlay-secondary" for="__secondary"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search this book..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<nav class="bd-header navbar navbar-expand-lg bd-navbar">
</nav>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<a class="navbar-brand logo" href="../landing.html">
<img alt="Logo image" class="logo__image only-light" src="../_static/logo.png"/>
<script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
</a></div>
<div class="sidebar-primary-item"><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../landing.html">
                    About this web
                </a>
</li>
</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../courses/courses.html">Courses</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../courses/introduction.html">Python Courses for beginners</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/table_content.html">1. Introduction to Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_2.html">2. Python Data Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_3.html">3. Control Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_4.html">4. Python Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_5.html">5. Lists, Tuples, Dictionary, and Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/study_guide_6.html">6. String</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/simple_flask_app.html">7. Create a Simple Flask App</a></li>
<li class="toctree-l2"><a class="reference internal" href="../courses/update_simple_flask_app.html">Update Flask App with more functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../blogs/blogs.html">Blogs</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../blogs/wealth.html">The Art of Leveraging Specific Knowledge</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/delta_table.html">Exploring Delta Lake’s Powerful Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/big_data.html">Harnessing the Power of Big Data: A Modern Marvel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blogs/cloud_services.html">A Overview of Azure, Google Cloud, and AWS</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="tests.html">Databricks</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="test_1.html">Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="answers_1.html">Correct Answers</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Explanations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mongol/main.html">Монгол</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../mongol/blog_1.html">Databricks гэж юу?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mongol/blog_2.html">Дата дата л гэнэ яг юу юм бэ?</a></li>
</ul>
</li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-bars"></span>
</label></div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="article-header-buttons">
<div class="dropdown dropdown-launch-buttons">
<button aria-expanded="false" aria-label="Launch interactive content" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fas fa-rocket"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="https://colab.research.google.com/github/byambaa1982/python_cources_for_beginers/blob/main/ebook/explaination_1.ipynb" target="_blank" title="Launch onColab">
<span class="btn__icon-container">
<img src="../_static/images/logo_colab.png"/>
</span>
<span class="btn__text-container">Colab</span>
</a>
</li>
</ul>
</div>
<div class="dropdown dropdown-source-buttons">
<button aria-expanded="false" aria-label="Source repositories" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fab fa-github"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm btn-source-repository-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="https://github.com/byambaa1982/python_cources_for_beginers" target="_blank" title="Source repository">
<span class="btn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="btn__text-container">Repository</span>
</a>
</li>
<li><a class="btn btn-sm btn-source-issues-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="https://github.com/byambaa1982/python_cources_for_beginers/issues/new?title=Issue%20on%20page%20%2Febook/explaination_1.html&amp;body=Your%20issue%20content%20here." target="_blank" title="Open an issue">
<span class="btn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
</ul>
</div>
<div class="dropdown dropdown-download-buttons">
<button aria-expanded="false" aria-label="Download this page" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fas fa-download"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm btn-download-source-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="../_sources/ebook/explaination_1.ipynb" target="_blank" title="Download source file">
<span class="btn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="btn btn-sm btn-download-pdf-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" onclick="window.print()" title="Print to PDF">
<span class="btn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
<button class="btn btn-sm btn-fullscreen-button" data-bs-placement="bottom" data-bs-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="btn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__secondary" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</label>
</div></div>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>Explanations</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1">Question 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2">Question 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-3">Question 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-4">Question 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-5">Question 5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-6">Question 6</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-7">Question 7</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-8">Question 8</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-9">Question 9</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-10">Question 10</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-11">Question 11</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-12">Question 12</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-13">Question 13</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-14">Question 14</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-15">Question 15</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-16">Question 16</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-17">Question 17</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-18">Question 18</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-19">Question 19</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-20">Question 20</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-21">Question 21</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-22">Question 22</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-23">Question 23</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-24">Question 24</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-25">Question 25</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-26">Question 26</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-27">Question 27</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-28">Question 28</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-29">Question 29</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-30">Question 30</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-31">Question 31</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-32">Question 32</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-33">Question 33</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-34">Question 34</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-35">Question 35</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-36">Question 36</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-37">Question 37</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-38">Question 38</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-39">Question 39</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-40">Question 40</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-41">Question 41</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-42">Question 42</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-43">Question 43</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-44">Question 44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-45">Question 45</a></li>
</ul>
</nav>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" role="main">
<section class="tex2jax_ignore mathjax_ignore" id="explanations">
<h1>Explanations<a class="headerlink" href="#explanations" title="Permalink to this heading">#</a></h1>
<section id="question-1">
<h2>Question 1<a class="headerlink" href="#question-1" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>E. A data lakehouse enables both batch and streaming analytics.</p></li>
</ul>
<p>Explanation:</p>
<p>A significant advantage of a data lakehouse over a traditional data warehouse is its ability to handle both batch and streaming analytics. Traditional data warehouses were primarily designed for batch processing, and they might struggle with real-time data ingestion and streaming analytics. On the other hand, a data lakehouse can accommodate both without any significant performance impact. The other options either are capabilities of both systems or don’t accurately describe the features of a data lakehouse.</p>
</section>
<section id="question-2">
<h2>Question 2<a class="headerlink" href="#question-2" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. Data plane</p></li>
</ul>
<p>Explanation:</p>
<p>The Data Plane in Databricks hosts the driver and worker nodes of a Databricks-managed cluster. It is responsible for the execution of jobs and computation tasks. The Control Plane, on the other hand, is where the user interface, REST API, job launching, and cluster management operations take place. The Databricks Filesystem, JDBC data source, and Databricks web application are not physical locations, so they don’t host driver or worker nodes.</p>
</section>
<section id="question-3">
<h2>Question 3<a class="headerlink" href="#question-3" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>D. A data lakehouse stores unstructured data and is ACID-compliant.</p></li>
</ul>
<p>Explanation:</p>
<p>A data lakehouse, such as Delta Lake, can meet the needs of both workloads because it can handle unstructured data (like video files for machine learning workloads) and is ACID-compliant (which is a key requirement for highly audited batch ETL/ELT workloads). ACID compliance ensures transactional reliability, which is essential for ETL/ELT processes.</p>
<p>The other options do not fully capture the dual functionality that a data lakehouse can provide for both types of workloads. Option B and C are incorrect because even though they are features of a data lakehouse, they do not directly address the needs of both machine learning and ETL/ELT workloads. Option A is incorrect because data lakehouses do require data modeling. Lastly, option E is also incorrect because a data lakehouse does not have to fully exist in the cloud - it can be implemented on-premises or in a hybrid environment as well.</p>
</section>
<section id="question-4">
<h2>Question 4<a class="headerlink" href="#question-4" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>C. An automated workflow needs to be run every 30 minutes.</p></li>
</ul>
<p>Explanation:</p>
<p>Job clusters are specifically designed for running scheduled or automated tasks, like an automated workflow that needs to run every 30 minutes (option C). They are spun up when the job starts and are terminated when the job finishes, thus providing efficient use of resources for tasks that do not require a constantly running cluster.</p>
<p>On the other hand, all-purpose clusters are meant for interactive analysis and collaboration - they are not the best choice for running scheduled jobs, as they are not automatically terminated when a task finishes and therefore could incur unnecessary costs if not manually shut down. This is why options A, B, D, and E are not the best use cases for job clusters.</p>
</section>
<section id="question-5">
<h2>Question 5<a class="headerlink" href="#question-5" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>C. Data Explorer</p></li>
</ul>
<p>Explanation:</p>
<p>Data Explorer in Databricks is the place where data engineers or data scientists can manage the permissions on tables. You can grant, revoke, or list permissions on your Delta tables for individual users or groups. In this scenario, the data engineer would use Data Explorer to give SELECT permission on the Delta table to the data analysts.</p>
<p>Other Options Explanation:</p>
<ul class="simple">
<li><p>A. Repos: Databricks Repos are used for version control and collaboration on code in notebooks, not for managing permissions on Delta tables.</p></li>
<li><p>B. Jobs: The Databricks Jobs service is used for scheduling and running jobs, not for managing permissions on Delta tables.</p></li>
<li><p>D. Databricks Filesystem: Databricks Filesystem (DBFS) is a distributed file system installed on Databricks clusters. It’s used for storing data, not for managing permissions on Delta tables.</p></li>
<li><p>E. Dashboards: Dashboards in Databricks are used for visualizing and sharing results, not for managing permissions on Delta tables.</p></li>
</ul>
</section>
<section id="question-6">
<h2>Question 6<a class="headerlink" href="#question-6" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>B. Databricks Notebooks support real-time coauthoring on a single notebook</p></li>
</ul>
<p>Explanation:</p>
<p>Databricks Notebooks support real-time coauthoring, which allows multiple users to simultaneously work on the same notebook, thus enhancing collaboration. Each user can see the other’s updates in real time. This feature makes it more efficient for the two junior data engineers to collaborate on the same notebook compared to working on separate Git branches and subsequently merging changes. Options A, C, D, and E, while true capabilities of Databricks Notebooks, do not directly address the specific collaboration scenario in question.</p>
</section>
<section id="question-7">
<h2>Question 7<a class="headerlink" href="#question-7" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>E. Databricks Repos can commit or push code changes to trigger a CI/CD process.</p></li>
</ul>
<p>Explanation:</p>
<p>Databricks Repos can be used to connect a notebook to a Git repository, which facilitates version control and continuous integration/continuous delivery (CI/CD) workflows. When changes are committed or pushed to the Git repository, this can trigger a CI/CD process, such as automated testing or deployment. While Databricks Repos does interact with Git branches (Option B) and serves as a link between Databricks and your Git repositories (Option D), it doesn’t facilitate the pull request, review, and approval process (Option A), merge changes (Option B), or trigger Git automation pipelines (Option C).</p>
</section>
<section id="question-8">
<h2>Question 8<a class="headerlink" href="#question-8" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>B. Delta Lake is an open format storage layer that delivers reliability, security, and
performance.</p></li>
</ul>
<p>Explanation:</p>
<p>Delta Lake is an open-source storage layer that brings ACID (Atomicity, Consistency, Isolation, Durability) transactional capabilities to Apache Spark and big data workloads. It provides capabilities such as schema enforcement and evolution, data reliability, and high performance, allowing for more reliable and performant big data processing and analytics. While it does store data (Option D), it is not just a data storage format, it enhances data reliability and performance. It is not an analytics engine (Option A), a platform for managing machine learning lifecycles (Option C), nor does it directly process data (Option E).</p>
</section>
<section id="question-9">
<h2>Question 9<a class="headerlink" href="#question-9" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>B. CREATE OR REPLACE TABLE table_name (
id STRING,
birthDate DATE,
avgRating FLOAT
)</p></li>
</ul>
<p>Explanation:</p>
<p>In this case, the correct answer is B because it correctly uses the CREATE OR REPLACE TABLE command and includes the column names and data types within parentheses. The other options either use the wrong keywords or incorrect syntax. Note that the USING DELTA keyword is not required here because the question doesn’t specify that a Delta table is needed. However, if a Delta table was required, the USING DELTA keyword could be added to the end of the command.</p>
</section>
<section id="question-10">
<h2>Question 10<a class="headerlink" href="#question-10" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>C. INSERT INTO</p></li>
</ul>
<p>Explanation:</p>
<p>The “INSERT INTO” statement is used in SQL to insert new records in a table. This is true for both traditional SQL databases and Delta Lake tables in Databricks.</p>
<p>The other options are not used for appending new rows to a table:</p>
<ul class="simple">
<li><p>UPDATE: This is used to modify existing records in a table.</p></li>
<li><p>COPY: This is not a standard SQL command to insert new records into a table.</p></li>
<li><p>DELETE: This is used to remove records from a table.</p></li>
<li><p>UNION: This is used to combine rows from two or more tables based on a related column between them, not to append new records into a single table.</p></li>
</ul>
</section>
<section id="question-11">
<h2>Question 11<a class="headerlink" href="#question-11" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>B. Z-Ordering</p></li>
</ul>
<p>Explanation:</p>
<p>Z-ordering is a technique in Delta Lake that co-locates related information in the same set of files. Z-ordering maps multidimensional data to one dimension while preserving the locality of the data points. A primary use-case for Z-ordering is for queries that contain a filter on the dimension columns. In such cases, reading the table causes only a minimum amount of data to be read thus improving the query’s performance.</p>
<ul class="simple">
<li><p>Data skipping is an optimization where a query skips over files whose metadata indicate they don’t match a query’s predicate.</p></li>
<li><p>Bin-packing is an optimization that combines small files into larger ones for more efficient reads.</p></li>
<li><p>Writing as a Parquet file is not an optimization technique, it is a file format.</p></li>
<li><p>Tuning the file size, while it can improve the query performance, it would not help in this case as the data that meets the condition is scattered throughout the data files.
So, considering all options, Z-Ordering is the most suitable answer.</p></li>
</ul>
</section>
<section id="question-12">
<h2>Question 12<a class="headerlink" href="#question-12" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>C. CREATE DATABASE IF NOT EXISTS customer360 LOCATION ‘/customer/customer360’;</p></li>
</ul>
<p>Explanation:</p>
<p>In Databricks, databases are used to organize tables into logical groups. The statement CREATE DATABASE IF NOT EXISTS creates a database with a specified name if it doesn’t already exist. The LOCATION clause is used to specify the DBFS path where the database is stored. Here, the requirement is to create a database named customer360 at the location /customer/customer360, and to avoid an error if the database already exists. Thus, the SQL command that fulfills this requirement is CREATE DATABASE IF NOT EXISTS customer360 LOCATION ‘/customer/customer360’;.</p>
<p>The other options do not satisfy all of these conditions. Specifically, options A and E will cause an error if the database already exists, and options B and D don’t specify the correct location. The DELTA keyword in options D and E is not used in the CREATE DATABASE statement in Databricks.</p>
</section>
<section id="question-13">
<h2>Question 13<a class="headerlink" href="#question-13" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>E. CREATE TABLE my_table (id STRING, value STRING);</p></li>
</ul>
<p>Explanation:</p>
<p>In Databricks, if you don’t specify a LOCATION when you’re creating a table, it becomes a managed table by default and its data and metadata are stored in the Databricks Filesystem (DBFS). This is true even if you don’t explicitly use the term “MANAGED” in your CREATE TABLE command.</p>
<p>So, the command CREATE TABLE my_table (id STRING, value STRING); will create a managed table with the data and metadata stored in DBFS, satisfying the requirements.</p>
</section>
<section id="question-14">
<h2>Question 14<a class="headerlink" href="#question-14" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. View</p></li>
</ul>
<p>Explanation:</p>
<p>A View in SQL is a virtual table based on the result-set of an SQL statement. A view contains rows and columns, just like a real table. The fields in a view are fields from one or more real tables in the database. It does not store physical data and is used to simplify complex queries, secure data, and present exactly the data that users are authorized to see.</p>
<p>On the other hand, a Temporary View is a view that is visible only to the current session, so it won’t be available to other data engineers in other sessions. Delta Tables, Databases, and Spark SQL Tables involve storing physical data.</p>
</section>
<section id="question-15">
<h2>Question 15<a class="headerlink" href="#question-15" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. The tables should be converted to the Delta format</p></li>
</ul>
<p>Explanation:</p>
<p>Delta Lake, a storage layer that brings ACID transactions to Apache Spark and big data workloads, would indeed help in this situation.</p>
<p>When using Delta Lake, you can leverage features such as upserts and deletes to modify data, schema evolution to easily add/remove/change a column’s type, and a history of operations performed on a table for auditability.</p>
<p>Furthermore, Delta Lake supports automatic statistics collection which helps with data skipping and improves the performance of queries. Delta Lake also maintains a transaction log that efficiently tracks changes to a dataset. This allows queries to always have a consistent view of the data, even while it is being modified. So, converting the tables to Delta format would ensure up-to-date data in the scenario given.</p>
</section>
<section id="question-16">
<h2>Question 16<a class="headerlink" href="#question-16" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. CREATE TABLE AS SELECT statements adopt schema details from the source
table and query.</p></li>
</ul>
<p>Explanation:</p>
<p>CREATE TABLE AS SELECT statements adopt schema details from the source table and query. In this SQL command, the new table’s schema is automatically determined based on the result of the SELECT statement. The types and number of columns in the new table will be derived from the SELECT statement used to populate it. In this case, the new table will have two columns: “country” and “customers”. The “country” column type would be the same as the corresponding column in the original table, and “customers” would be of type INT, as it’s a result of the COUNT function.</p>
</section>
<section id="question-17">
<h2>Question 17<a class="headerlink" href="#question-17" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>B. Overwriting a table results in a clean table history for logging and audit purposes.</p></li>
</ul>
<p>Explanation:</p>
<p>Overwriting a table does not result in a clean table history for logging and audit purposes. In fact, Delta Lake’s versioning capabilities (known as “Time Travel”) keep a record of every transaction made to the table. This includes overwrites, which can be queried and examined at any point in time. Therefore, overwriting a table does not remove prior versions or create a fresh history. It rather adds a new version to the existing history, enabling users to “travel back in time” and access previous versions of the data. The other statements correctly describe some of the advantages of overwriting a table instead of deleting and recreating it.</p>
</section>
<section id="question-18">
<h2>Question 18<a class="headerlink" href="#question-18" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>C. SELECT DISTINCT * FROM my_table;</p></li>
</ul>
<p>Explanation:</p>
<p>The SQL command SELECT DISTINCT * FROM my_table; is used to return unique records from a table, effectively removing duplicates. DISTINCT is a clause in SQL that allows you to eliminate duplicates from the result set of a query. It can work on a single column or multiple columns to find unique tuples (rows) in your table. The other options either do not use valid SQL commands (A, B) or don’t inherently remove duplicates from a result set (D, E).</p>
</section>
<section id="question-19">
<h2>Question 19<a class="headerlink" href="#question-19" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. INNER JOIN</p></li>
</ul>
<p>Explanation:</p>
<p>The SQL command INNER JOIN is used to combine rows from two or more tables based on a related column between them, which meets the requirement of only including rows whose value in the key column is present in both tables.</p>
<p>OUTER JOIN would include records from both tables, even if there is no match in the key column in one of the tables. LEFT JOIN would return all records from the left table, and the matched records from the right table. MERGE is used to combine rows from two tables based on a related column between them, especially when you want to update or insert data in bulk. UNION is used to combine the result-set of two or more SELECT statements.</p>
</section>
<section id="question-20">
<h2>Question 20<a class="headerlink" href="#question-20" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>D. SELECT cart_id, explode(items) AS item_id FROM raw_table;</p></li>
</ul>
<p>Explanation:</p>
<p>In Apache Spark SQL, the explode function is used to create a new row for each element in the given array or map column. Here, the explode function will create a new row for each item_id in the items array for each cart_id, which results in the desired schema with cart_id and item_id as separate columns.</p>
<p>The other options (filter, flatten, reduce, slice) are not suitable for this task. They perform different functions and would not result in the desired schema.</p>
</section>
<section id="question-21">
<h2>Question 21<a class="headerlink" href="#question-21" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>B. SELECT transaction_id, payload.date FROM raw_table;</p></li>
</ul>
<p>Explanation:</p>
<p>In this case, the payload is an array of structures (each structure contains customer_id, date, and store_id). To access individual fields in the structure, the “.” (dot) notation is used. So, payload.date will extract the date of each transaction from the payload array.</p>
<p>The SELECT statement in option B retrieves the transaction_id and the date from each record in the raw_table, which results in the desired schema.</p>
<p>The other options would not result in the correct schema because they either attempt to use functions or notations incorrectly (e.g., explode, array index notation), or they don’t attempt to extract the date from the payload structure at all.</p>
</section>
<section id="question-22">
<h2>Question 22<a class="headerlink" href="#question-22" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. They could wrap the query using PySpark and use Python’s string variable system to
automatically update the table name.</p></li>
</ul>
<p>Explanation:</p>
<p>The best way to automate this process would be to use PySpark and Python’s string variable system to update the table name dynamically. By using Python’s date and time functionality, the team could automate the insertion of the current date into the query.</p>
<p>The other options either involve manual intervention (Option B), are unlikely to meet the data analyst’s needs (Option C), or don’t address the requirement of updating the table name with the current date (Options D and E).</p>
</section>
<section id="question-23">
<h2>Question 23<a class="headerlink" href="#question-23" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. raw_df.createOrReplaceTempView(“raw_df”)</p></li>
</ul>
<p>Explanation:</p>
<p>The method createOrReplaceTempView creates (or replaces if that view name already exists) a lazily evaluated “view” that can be used like a table in Spark SQL. It does not persist to memory and is only available in this Spark session. This makes it ideal for temporary data sharing or examination. The other options don’t create a temporary view or are not valid methods for a DataFrame. Option E is incorrect as Spark SQL and PySpark can share data through views or tables.</p>
</section>
<section id="question-24">
<h2>Question 24<a class="headerlink" href="#question-24" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>D. f”{region}{store}<em>sales</em>{year}”</p></li>
</ul>
<p>Explanation:</p>
<p>In Python, f-strings are a method of string interpolation. They allow expressions to be embedded inside string literals, using curly braces {}. The expressions will be replaced with their values when the string is created. The f-string syntax f”{region}{store}<em>sales</em>{year}” will replace {region}, {store}, and {year} with the values of those variables, resulting in a string like nyc100_sales_2021. The other options will not correctly format the string. For instance, option A, C and E will not replace the variables with their values, and option B will include unnecessary ‘+’ characters in the output string.</p>
</section>
<section id="question-25">
<h2>Question 25<a class="headerlink" href="#question-25" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. The .read line should be replaced with .readStream</p></li>
</ul>
<p>Explanation:</p>
<p>Spark offers two kinds of data processing operations: batch and streaming. The .read operation is used for batch processing, where data is read from a static source. The .readStream operation, on the other hand, is used for real-time processing, where data is continuously read from a streaming source.</p>
<p>Therefore, if a data engineer wants to perform a streaming read from a data source, they should use the .readStream operation instead of the .read operation. The corrected code block would look like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="p">(</span><span class="n">spark</span>
<span class="o">.</span><span class="n">readStream</span>
<span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
<span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">"cloudFiles"</span><span class="p">)</span>
<span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"cloudFiles.format"</span><span class="p">,</span> <span class="s2">"json"</span><span class="p">)</span>
<span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dataSource</span><span class="p">)</span>
<span class="p">)</span>

</pre></div>
</div>
<p>This correction should resolve the error, and the data engineer will be able to perform a streaming read from the data source successfully.</p>
</section>
<section id="question-26">
<h2>Question 26<a class="headerlink" href="#question-26" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. trigger(once=True)</p></li>
</ul>
<p>Explanation:</p>
<p>Structured Streaming in Spark provides a high-level API for stream processing. To specify how often the streaming computation should be run, we use the trigger setting.</p>
<p>If we want the query to execute only a single batch of data, we can use trigger(once=True). This will process all available data as a single batch and then terminate the operation. This is known as a one-time trigger.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">"sales"</span><span class="p">)</span>
<span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">"avg_price"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s2">"sales"</span><span class="p">)</span> <span class="o">/</span> <span class="n">col</span><span class="p">(</span><span class="s2">"units"</span><span class="p">))</span>
<span class="o">.</span><span class="n">writeStream</span>
<span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"checkpointLocation"</span><span class="p">,</span> <span class="n">checkpointPath</span><span class="p">)</span>
<span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">"complete"</span><span class="p">)</span>
<span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">once</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">"new_sales"</span><span class="p">)</span>
<span class="p">)</span>

</pre></div>
</div>
</section>
<section id="question-27">
<h2>Question 27<a class="headerlink" href="#question-27" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>E. Auto Loader</p></li>
</ul>
<p>The data engineer can use Auto Loader to solve this problem. Auto Loader, a feature available in Databricks, is designed to simplify the process of incrementally ingesting data, such as new files in a directory. <code class="docutils literal notranslate"><span class="pre">Auto</span> <span class="pre">Loader</span></code> keeps track of new files as they arrive, and it automatically processes these files. It’s a robust and low-maintenance option for ingesting incrementally updated or new data.</p>
<p>Please note, while <code class="docutils literal notranslate"><span class="pre">Delta</span> <span class="pre">Lake</span> <span class="pre">(B)</span></code> is a technology that can provide features like ACID transactions, versioning, and schema enforcement on data lakes, it doesn’t directly address the problem of identifying and processing only new files since the last pipeline run. Similarly, <code class="docutils literal notranslate"><span class="pre">Databricks</span> <span class="pre">SQL</span> <span class="pre">(A)</span></code>, <code class="docutils literal notranslate"><span class="pre">Unity</span> <span class="pre">Catalog</span> <span class="pre">(C)</span></code>, and <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Explorer</span> <span class="pre">(D</span></code>) are not directly targeted at this specific problem.</p>
</section>
<section id="question-28">
<h2>Question 28<a class="headerlink" href="#question-28" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>C. There is no change required. The inclusion of <code class="docutils literal notranslate"><span class="pre">format("cloudFiles")</span></code> enables the use of Auto Loader.</p></li>
</ul>
<p>In Databricks, the Auto Loader feature is utilized through the “cloudFiles” format option. So, the given code block is already configured to use Auto Loader. Therefore, no changes are needed in the code.</p>
<p>Other Options Explanation:</p>
<ul class="simple">
<li><p>A. The data engineer needs to change the <code class="docutils literal notranslate"><span class="pre">format("cloudFiles")</span></code> line to <code class="docutils literal notranslate"><span class="pre">format("autoLoader")</span></code>: This is incorrect because Auto Loader is invoked using the “cloudFiles” format, not “autoLoader”.</p></li>
<li><p>B. There is no change required. Databricks automatically uses Auto Loader for streaming reads: This is incorrect. Auto Loader needs to be explicitly invoked in the readStream command using the “cloudFiles” format.</p></li>
<li><p>D. The data engineer needs to add the .autoLoader line before the <code class="docutils literal notranslate"><span class="pre">.load(sourcePath)</span></code> line: This is incorrect as there is no .autoLoader option available. The Auto Loader is enabled using the <code class="docutils literal notranslate"><span class="pre">format("cloudFiles")</span></code> command.</p></li>
<li><p>E. There is no change required. The data engineer needs to ask their administrator to turn on Auto Loader: This is incorrect. The Auto Loader can be enabled directly in the code by the data engineer without needing any administrative privileges.</p></li>
</ul>
</section>
<section id="question-29">
<h2>Question 29<a class="headerlink" href="#question-29" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>E. A job that enriches data by parsing its timestamps into a human-readable format.</p></li>
</ul>
<p>Explanation:</p>
<p>Bronze tables in Databricks’ Lakehouse pattern are used for storing raw, unprocessed data. However, the process of converting timestamps into a more human-readable format is a form of data enrichment, which typically happens at this “Bronze” level. The enriched data is then stored in Silver tables for further processing or analysis.</p>
<p>Other Options Explanation:</p>
<ul class="simple">
<li><p>A. A job that aggregates cleaned data to create standard summary statistics: This would likely utilize a Silver or Gold table, which contain processed and cleaned data ready for analysis.</p></li>
<li><p>B. A job that queries aggregated data to publish key insights into a dashboard: This would likely involve a Gold table, which contains data that has been cleaned, processed, and possibly aggregated, and is used for reporting and analytics.</p></li>
<li><p>C. A job that ingests raw data from a streaming source into the Lakehouse: This is a correct statement, however, it does not pertain to the task of parsing timestamps which the question focuses on.</p></li>
<li><p>D. A job that develops a feature set for a machine learning application: Depending on the specifics, this could potentially involve any stage of data, but typically this would be either Silver or Gold data, which has been cleaned and processed.</p></li>
</ul>
</section>
<section id="question-30">
<h2>Question 30<a class="headerlink" href="#question-30" title="Permalink to this heading">#</a></h2>
<p>D. A job that aggregates cleaned data to create standard summary statistics</p>
<p>Explanation:</p>
<p>In the Databricks Lakehouse paradigm, a Silver table is typically used to store clean and processed data that can be used for various types of transformations, including the calculation of aggregated statistics. Therefore, a job that aggregates cleaned data to create standard summary statistics would utilize a Silver table as its source.</p>
<p>Other Options Explanation:</p>
<p>A. A job that enriches data by parsing its timestamps into a human-readable format: This operation could be performed at either the Bronze or Silver level, depending on the specific requirements of the pipeline. However, as it’s a form of data cleaning or enrichment, it’s more likely to be performed at the Bronze level.</p>
<p>B. A job that queries aggregated data that already feeds into a dashboard: This type of operation typically involves Gold tables, which are used for high-level reporting and analytics.</p>
<p>C. A job that ingests raw data from a streaming source into the Lakehouse: This operation would involve a Bronze table, which is used for the initial ingestion of raw data.</p>
<p>E. A job that cleans data by removing malformatted records: While this could potentially occur at the Silver level, the initial round of data cleaning often happens at the Bronze level before the data is loaded into a Silver table.</p>
</section>
<section id="question-31">
<h2>Question 31<a class="headerlink" href="#question-31" title="Permalink to this heading">#</a></h2>
<p>C.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">"sales"</span><span class="p">)</span>
<span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">"avgPrice"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s2">"sales"</span><span class="p">)</span> <span class="o">/</span> <span class="n">col</span><span class="p">(</span><span class="s2">"units"</span><span class="p">))</span>
<span class="o">.</span><span class="n">writeStream</span>
<span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">"checkpointLocation"</span><span class="p">,</span> <span class="n">checkpointPath</span><span class="p">)</span>
<span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">"append"</span><span class="p">)</span>
<span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">"cleanedSales"</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Explanation:</p>
<p>The Bronze-Silver-Gold architecture in data management is a tiered framework for data processing and storage. Bronze represents raw, unprocessed data, Silver represents cleansed and enriched data, while Gold represents aggregated and business-ready data.</p>
<p>The statement under option C takes a stream of data from the “sales” Bronze table, enriches it by calculating the “avgPrice”, and writes it to the “cleanedSales” table, effectively transforming it to a Silver table. The transition from raw data to enriched data is generally considered a transition from a Bronze to a Silver table, which is why this option is the correct answer.</p>
</section>
<section id="question-32">
<h2>Question 32<a class="headerlink" href="#question-32" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. The ability to declare and maintain data table dependencies</p></li>
</ul>
<p>Explanation:</p>
<p>Delta Live Tables offers several advantages over standard data pipelines that use Spark and Delta Lake on Databricks. One such advantage is the ability to declare and maintain dependencies between tables. This is useful when the output of one table is used as the input to another. It makes the pipeline more maintainable and resilient to changes.</p>
<p>Option B, the ability to write pipelines in Python and/or SQL, is not unique to Delta Live Tables. You can also write Spark and Delta Lake pipelines in these languages.</p>
<p>Option C, accessing previous versions of data tables, is a feature of Delta Lake (through Delta Time Travel), not specific to Delta Live Tables.</p>
<p>Option D, the ability to automatically scale compute resources, is a feature of Databricks, not specific to Delta Live Tables.</p>
<p>Option E, performing batch and streaming queries, is possible with both Spark Structured Streaming and Delta Lake, not specific to Delta Live Tables.</p>
</section>
<section id="question-33">
<h2>Question 33<a class="headerlink" href="#question-33" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>B. They need to create a Delta Live Tables pipeline from the Jobs page.</p></li>
</ul>
<p>Explanation:</p>
<p>Here are the steps on how to create a Delta Live Tables pipeline from the Jobs page in Databricks:</p>
<ol class="arabic simple">
<li><p>Go to the Jobs page in Databricks.</p></li>
<li><p>Click on the “Create Job” button.</p></li>
<li><p>Select “Delta Live Tables” as the job type.</p></li>
<li><p>Select the notebooks that you want to include in the pipeline.</p></li>
<li><p>Specify the order in which you want the notebooks to be executed.</p></li>
<li><p>Click on the “Create” button.</p></li>
</ol>
</section>
<section id="question-34">
<h2>Question 34<a class="headerlink" href="#question-34" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>B. They need to add a CREATE LIVE TABLE table_name AS line at the beginning of the query.</p></li>
</ul>
<p>Explanation:</p>
<p>Delta Live Tables (DLT) supports SQL and Python syntax. To define a table in SQL, one has to use the CREATE LIVE TABLE statement before defining the query. Hence, to convert the query into a DLT compatible query, we would add the CREATE LIVE TABLE command at the beginning.</p>
<p>It’s worth mentioning that as of my knowledge cutoff in September 2021, this is the correct information. Always refer to the most recent Databricks documentation for the most up-to-date processes and procedures.</p>
</section>
<section id="question-35">
<h2>Question 35<a class="headerlink" href="#question-35" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.</p></li>
</ul>
<p>Explanation:</p>
<p>The CONSTRAINT clause in Delta Live Tables sets an expectation on the dataset. If the action to be taken upon violation of the expectation is not explicitly specified, the default action, which is ‘warn’, is applied. With the ‘warn’ action, the records that do not meet the expectation are written to the target dataset, and a failure metric for the dataset is recorded in the event log.</p>
</section>
<section id="question-36">
<h2>Question 36<a class="headerlink" href="#question-36" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.</p></li>
</ul>
<p>Explanation:</p>
<p>In Continuous Pipeline Mode with Production mode enabled, all tables in the pipeline are updated continuously until the pipeline is manually stopped. When ‘Start’ is clicked, the pipeline initiates and continues to run, processing incoming data in real-time. Compute resources are allocated for the entire duration of the pipeline execution, thus ensuring optimal performance.</p>
</section>
<section id="question-37">
<h2>Question 37<a class="headerlink" href="#question-37" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>D. They can institute a retry policy for the task that periodically fails</p></li>
</ul>
<p>Explanation:</p>
<p>A retry policy can be instituted at the task level in Databricks. This would allow the specific task that is failing to be retried without having to rerun the entire job, thus minimizing compute costs. The other options, such as retrying the entire job or setting the job to run multiple times, would increase compute costs as more tasks would be run than necessary. Observing the task as it runs might help determine why it is failing, but it won’t ensure the job completes each night. Finally, utilizing a jobs cluster for each of the tasks in the job would not necessarily address the issue of the task failing and could lead to increased compute costs.</p>
</section>
<section id="question-38">
<h2>Question 38<a class="headerlink" href="#question-38" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. They can utilize multiple tasks in a single job with a linear dependency</p></li>
</ul>
<p>Explanation:</p>
<p>The most reliable solution to this problem would be to set up multiple tasks within a single job with a linear dependency. This approach ensures that the second task (which was previously the second job) will not start until the first task (previously the first job) has successfully completed. This removes the problem of the second job starting before the first has completed. The other options, like using cluster pools, setting a retry policy on the first job, or limiting the size of the output in the second job, do not directly address the issue of the second job starting before the first job has finished. The option to set up the data to stream from the first job to the second job is not a typical way to ensure job dependencies in Databricks.</p>
</section>
<section id="question-39">
<h2>Question 39<a class="headerlink" href="#question-39" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>C. They can download the JSON description of the Job from the Job’s page.</p></li>
</ul>
<p>Explanation:</p>
<p>Databricks provides the option to download a JSON description of the job configuration from the job’s page. This JSON description can be version controlled, and can also be used to recreate the job programmatically using the Databricks Jobs API. This provides a way to have version-controllable configuration of the Job’s schedule. Other options like linking the job to notebooks that are a part of a Databricks Repo, submitting the job on a job cluster or all-purpose cluster, or downloading an XML description of the job do not offer the same level of version control for a job’s schedule.</p>
</section>
<section id="question-40">
<h2>Question 40<a class="headerlink" href="#question-40" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>C. They can increase the cluster size of the SQL endpoint.</p></li>
</ul>
<p>Explanation:</p>
<p>Databricks SQL Endpoint is the compute that is used to execute SQL queries in Databricks. The performance of the SQL queries can be directly improved by increasing the size of the underlying cluster used by the SQL endpoint. The cluster size in Databricks is defined by the number and type of machines (known as nodes) in the cluster. A larger cluster can handle more data and perform operations faster because the computations are distributed among more machines. Therefore, by increasing the cluster size of the SQL endpoint, the data engineering team can improve the performance of the data analyst’s queries. Other options mentioned might not have a direct impact on the performance of the SQL queries.</p>
</section>
<section id="question-41">
<h2>Question 41<a class="headerlink" href="#question-41" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>B. They can schedule the query to refresh every 1 day from the query’s page in
Databricks SQL.</p></li>
</ul>
<p>Explanation:</p>
<p>In Databricks SQL, users have the ability to schedule their SQL queries to run at specific intervals. This can be very useful for scenarios where the results of the query need to be updated periodically. In this case, the engineering manager can schedule the query to refresh every 1 day directly from the query’s page in Databricks SQL. This will ensure that the query is executed automatically every day, updating the results without requiring the manager to manually rerun the query each time. Other options such as scheduling from the Jobs UI or from the SQL endpoint’s page may not be applicable as they pertain to different Databricks features.</p>
</section>
<section id="question-42">
<h2>Question 42<a class="headerlink" href="#question-42" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>D. They can set up an Alert for the query to notify them if the returned value is greater
than 60.</p></li>
</ul>
<p>Explanation:</p>
<p>Databricks SQL allows you to create Alerts on specific queries. In this scenario, the data engineering team can create an Alert on the query that monitors the ELT job runtime. The Alert can be set to trigger a notification when the query result (the number of minutes since the job’s most recent runtime) exceeds 60. This way, the team will be notified if the ELT job has not run in over an hour. Other options such as alerting on dashboard conditions or job failures may not provide the specific information needed in this case.</p>
</section>
<section id="question-43">
<h2>Question 43<a class="headerlink" href="#question-43" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>D. The Job associated with updating the dashboard might be using a non-pooled
endpoint.</p></li>
</ul>
<p>Explanation:</p>
<p>Databricks SQL dashboards do not directly depend on Jobs. Rather, they execute SQL queries on SQL Endpoints (either serverless or a cluster-backed endpoint). Therefore, the speed at which a dashboard refreshes is independent of whether the Jobs are using pooled or non-pooled endpoints. The other options, such as the SQL endpoint needing time to start up, the inherent complexity of the queries, or the queries checking for new data before executing, could indeed contribute to the delay in updating the dashboard. The fifth option, E, is incorrect because individual queries in a dashboard do not connect to their own, separate Databricks clusters.</p>
</section>
<section id="question-44">
<h2>Question 44<a class="headerlink" href="#question-44" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>C. GRANT SELECT ON TABLE sales TO <a class="reference external" href="mailto:new.engineer%40company.com">new<span>.</span>engineer<span>@</span>company<span>.</span>com</a>;</p></li>
</ul>
<p>Explanation:</p>
<p>The GRANT SELECT command is used to give a user permission to read a database table. Therefore, the command GRANT SELECT ON TABLE sales TO <a class="reference external" href="mailto:new.engineer%40company.com">new<span>.</span>engineer<span>@</span>company<span>.</span>com</a>; would grant the new data engineer the permissions needed to query the ‘sales’ table. The USAGE privilege doesn’t allow data access, it only allows the user to access objects in the database. CREATE privilege would allow the user to create objects in the database, but not necessarily read data from existing tables. Options D and E are incorrectly formulated, as they try to grant privileges on a table named “<a class="reference external" href="mailto:new.engineer%40company.com">new<span>.</span>engineer<span>@</span>company<span>.</span>com</a>” to a user or role called ‘sales’.</p>
</section>
<section id="question-45">
<h2>Question 45<a class="headerlink" href="#question-45" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A. GRANT ALL PRIVILEGES ON TABLE sales TO <a class="reference external" href="mailto:new.engineer%40company.com">new<span>.</span>engineer<span>@</span>company<span>.</span>com</a>;</p></li>
</ul>
<p>Explanation:</p>
<p>The GRANT ALL PRIVILEGES command is used to give a user full permissions to an object, such as a table. In this case, the command GRANT ALL PRIVILEGES ON TABLE sales TO <a class="reference external" href="mailto:new.engineer%40company.com">new<span>.</span>engineer<span>@</span>company<span>.</span>com</a>; would grant the new data engineer all necessary permissions to fully manage the ‘sales’ table. The other options either grant insufficient privileges (e.g., USAGE or SELECT only) or are not correctly formatted SQL commands.</p>
</section>
</section>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ebook"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="answers_1.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Correct Answers</p>
</div>
</a>
<a class="right-next" href="../mongol/main.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Монгол</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div></div>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> Contents
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1">Question 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2">Question 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-3">Question 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-4">Question 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-5">Question 5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-6">Question 6</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-7">Question 7</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-8">Question 8</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-9">Question 9</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-10">Question 10</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-11">Question 11</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-12">Question 12</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-13">Question 13</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-14">Question 14</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-15">Question 15</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-16">Question 16</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-17">Question 17</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-18">Question 18</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-19">Question 19</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-20">Question 20</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-21">Question 21</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-22">Question 22</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-23">Question 23</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-24">Question 24</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-25">Question 25</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-26">Question 26</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-27">Question 27</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-28">Question 28</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-29">Question 29</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-30">Question 30</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-31">Question 31</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-32">Question 32</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-33">Question 33</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-34">Question 34</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-35">Question 35</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-36">Question 36</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-37">Question 37</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-38">Question 38</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-39">Question 39</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-40">Question 40</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-41">Question 41</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-42">Question 42</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-43">Question 43</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-44">Question 44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#question-45">Question 45</a></li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
<div class="bd-footer-content__inner container">
<div class="footer-item">
<p class="component-author">
By Byamba Enkhbat
</p>
</div>
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2022.
      <br/>
</p>
</div>
<div class="footer-item">
</div>
<div class="footer-item">
</div>
</div>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>
<footer class="bd-footer">
</footer>
</body>
</html>